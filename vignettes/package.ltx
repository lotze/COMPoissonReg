%\VignetteIndexEntry{A Vignette for the COMPoissonReg Package}
%\VignetteAuthor{Andrew and ?}
%\VignetteKeyword{R}
%\VignetteKeyword{package}
%\VignetteKeyword{vignette}
%\VignetteKeyword{LaTeX}
%\VignetteEngine{R.rsp::tex}

\documentclass[12pt]{article}
\usepackage{common}

\title{A Vignette for the COMPoissonReg Package}
\author{Andrew and ?}
\date{2021-02-09}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:intro}
TBD
\begin{enumerate*}
\item This vignette gives an overview of the COMPoissonReg package.

\item The COMPoissonReg package fits COM-Poisson (CMP) and Zero-Inflated COM-Poisson (ZICMP) regression models.

\item A brief review of relevant literature (and whatever we want to mention).
%
\begin{enumerate*}
\item The distribution was first described in \citet{ConwayMaxwell1962}.
\item It found a renewed interest in count modeling in \citet{ShmueliEtAl2005} because of its ability to capture both over- and underdispersion.
\item Other interesting count distributions include Negative Binomial, Generalized Poisson, \ldots
\item CMP was explored for use in count regression models in \citet{SellersShmueli2010}.
\item ZICMP was explored for zero-inflated count regression in \citet{SellersRaim2016}.
\item Works by \citet{Huang2017} and \citet{RibeiroEtAl2020} reparameterize CMP to be more interpretable in terms of the mean.
\item Some other interesting extensions to CMP include the multivariate CMP distribution \citep{SellersMorrisBalakrishnan2016}, count processes based on CMP \citep{ZhuEtAl2017}, zero- and $k$-inflated CMP \citep{AroraChagantySellers2021}, the Conway Maxwell Binomial (CMB) regression model (TBD: cite), the Conway Maxwell Multinomial (CMM) regression model \citep{KadaneWang2018,MorrisRaimSellers2020}, \ldots
\end{enumerate*}


\item Normalizing constant.
%
\begin{enumerate*}
\item \citet{ShmueliEtAl2005} seem to be the first ones to address this. Others listed here are more sophisticated and build on this initial work. We don't use them in the software, but they may be interesting to readers.
\item \citet{GillispieGreen2015} 
\item \citet{DalyGaunt2016}
\item \citet{BrooksEtAl2017} apparently also has an adaptive method for computing the CMP normalizing constant.
\item \citet{GauntEtAl2019}
\end{enumerate*}

\item Related software packages.
%
\begin{enumerate*}
\item The \code{compoisson} R package \citep{compoisson} implements CMP distribution functions. It had been on CRAN since 2011, but was deprecated on 2/16/2020.
\item The \code{CompGLM} R package \citep{CompGLM} implements CMP regression. It had been on CRAN since 2014, but was deprecated on 10/16/2019.
\item CMP regression is supported in the SAS COUNTREG procedure \citep{SASProcCountreg2018}
\item The \code{mpcmp} R package \citep{FungEtAl2020} implements the mean-parameterized CMP regression described in \citet{Huang2017}.
\item The \code{cmpreg} R package \citep{cmpreg} implements the CMP parameterization from \citet{RibeiroEtAl2020}.
\item The \code{combayes} R package \citep{combayes} implements Bayesian CMP analysis.
\item The \code{multicmp} R package \citep{multicmp} supports the multivariate CMP distribution \citep{SellersMorrisBalakrishnan2016}.
\item The \code{COMMultReg} R package \citep{COMMultReg} supports the CMM regression model.
\end{enumerate*}

\item In Section~\ref{sec:model}, we briefly review the model. In Section~\ref{sec:normconst}, we discuss computation of the normalizing constant, \ldots
\end{enumerate*}

\section{Model}
\label{sec:model}
\paragraph{Conway Maxwell Poisson.}
Let $Y \sim \text{CMP}(\lambda, \nu)$ be a Conway Maxwell Poisson (CMP) random
variable with density
%
\begin{align*}
f(y \mid \lambda, \nu) = \frac{\lambda^y}{(y!)^\nu Z(\lambda, \nu)}, \quad y = 0, 1, \ldots, \quad
Z(\lambda, \nu) = \sum_{r=0}^\infty \frac{\lambda^r}{(r!)^\nu},
\end{align*}
%
where $\lambda > 0$ and $\nu > 0$. $\text{CMP}(\lambda, \nu)$ is a
natural exponential family with
%
\begin{align*}
f(y) &= \exp\{ y \log \lambda - \nu \log(y!) - \log Z(\lambda, \nu) \} \\
&= \exp\{ a(y) - b(\eta) + \vec{s}(y)^\top \vec{\eta}  \},
\end{align*}
%
with natural parameter $\vec{\eta} = (\log \lambda, \nu)$, sufficient
statistic
$\vec{s}(y) = (y, -\log(y!))$, $b(\vec{\eta}) = \log Z(e^{\eta_1}, \eta_2)$,
and $a(y) = 0$. For expected value and variance, we have
%
\begin{align*}
\E(Y) &= \lambda \frac{\partial}{\partial \lambda} \log Z(\lambda, \nu), \\
\Var(Y) &= \lambda^2 \frac{\partial^2}{\partial \lambda^2} \log Z(\lambda, \nu) + \E(Y)
\end{align*}
%
Three notable special cases of $\text{CMP}(\lambda, \nu)$ help to demonstrate its flexibility in capturing dispersion. The case $\nu = 1$ corresponds to $\text{Poisson}(\lambda)$. When $\lambda \in (0,1)$ and $\nu \rightarrow 0$, the $\text{CMP}(\lambda, \nu)$ distribution becomes $\text{Geometric}(\lambda)$ with density $f(y \mid \lambda) = \lambda^y (1 - \lambda)^{1-y}$ for $y = 0, 1, \ldots$, which is overdispersed relative to Poisson. When $\nu \rightarrow \infty$, $\text{CMP}(\lambda, \nu)$ converges to a $\text{Bernoulli}(\lambda / (1 + \lambda))$ distribution which is underdispersed relative to Poisson.

\paragraph{Zero-Inflated Conway Maxwell Poisson.}
Let $S \sim \text{Bernoulli}(p)$ and $T \sim \text{CMP}(\lambda, \nu)$ be independent random variables. Then $Y = (1-S) T$ has Zero-Inflated Conway Maxwell Poisson distribution $\text{ZICMP}(\lambda, \nu, p)$ with density
%
\begin{align*}
f(y \mid \lambda, \nu, p) =  (1 - p) \frac{\lambda^{y}}{(y!)^{\nu} Z(\lambda, \nu)}
+ p \cdot \ind(y = 0).
\end{align*}
%
Many of the properties of ZICMP are apparent from its CMP construction; for example
%
\begin{align*}
\E(Y) &= (1-p) \E(T), \\
\Var(Y) &= (1-p) \left\{ \Var(T) + p[\E(T)]^2 \right\}.
\end{align*}
%
Like CMP, several interesting special cases are obtained by varying $\nu$. 
%
\begin{itemize}
\item[(a)] Taking $\nu = 1$ corresponds to Zero-Inflated Poisson $\text{ZIP}(\lambda, p)$ with density $f(y \mid \lambda, p) = (1 - p) e^{-\lambda} \lambda^{y} / y!
+ p \cdot \ind(y = 0)$.

\item[(b)] When $\lambda \in (0,1)$ and $\nu \rightarrow 0$, $\text{ZICMP}(\lambda, \nu)$ converges to a Zero-Inflated Geometric distribution with density $f(y \mid \lambda, p) = (1 - p) \lambda^y (1 - \lambda)^{1-y} + p \cdot \ind(y = 0)$ for $y = 0, 1, \ldots$.

\item[(c)] Finally, when $\nu \rightarrow \infty$, $\text{ZICMP}(\lambda, \nu, p)$ converges to a ``Zero-Inflated Bernoulli'' distribution with density 
%
\begin{math}
f(y \mid \lambda, p) = (1 - p) \left[ \lambda/(1+\lambda) \right]^y \left[ 1/(1+\lambda) \right]^{1-y} + p \cdot \ind(y = 0).
\end{math}
\end{itemize}

In case~(c) the $\lambda$ and $p$ parameters are not identifiable, and users may want to avoid ZICMP in data analyses with extreme underdispersion.

\paragraph{ZICMP Regression Model.}
Suppose there are $n$ subjects with count-valued outcomes $y_1, \ldots, y_n$
and covariates $\vec{x}_i \in \mathbb{R}^{d_1}$, $\vec{s}_i \in
\mathbb{R}^{d_2}$, and $\vec{w}_i \in \mathbb{R}^{d_3}$ for $i = 1, \ldots, n$.
The \pkg{COMPoissonReg} package fits Zero-Inflated COM-Poisson regression
models of the form
%
\begin{align*}
Y_i \indep \text{ZICMP}(\lambda_i, \nu_i, p_i), \quad i = 1, \ldots, n,
\end{align*}
%
where $\log \lambda_i = \vec{x}_i^\top \vec{\beta}$, $\log \nu_i = \vec{s}_i^\top \vec{\gamma}$, and $\logit p_i = \vec{w}_i^\top \vec{\zeta}$. Writing $\btheta = (\vec{\beta}, \vec{\gamma}, \vec{\zeta})$, the likelihood is
%
\begin{align}
L(\btheta) =
\prod_{i=1}^n \left[(1 - p_i)
\frac{\lambda_i^{y_i}}{(y_i!)^{\nu_i} Z(\lambda_i, \nu_i)}
+ p_i \ind(y_i = 0)
\right].
\label{eqn:likelihood}
\end{align}

(TBD: mention that, if possible, we will try to discuss in terms of the ZICMP model, for which all the other models are special cases.)

\begin{itemize*}
\item Demonstrate functions for the CMP and ZICMP distributions. This should include \code{d}, \code{p}, \code{q}, and \code{r} functions, \code{e} and \code{v}, and the normalizing constant.
\item To do this, we maximize the log-likelihood function in optim
\item We provide some hooks to control optim
\item Formula interface
\item We might need a section on handling large counts in the regression model. This is especially where numerical problems tend to occur.
\item \citet{Eddelbuettel2013} for Rcpp ...
\item For normalizing constant, can we somehow give lower bounds so we don't need to start summing from 0? Is it possible to use root-finding here since the bounds aren't quite invertible themselves?
\item Maybe give code in Section~\ref{sec:normconst} to change the options for tolerances?
\end{itemize*}


\section{Normalizing Constant}
\label{sec:normconst}
The normalizing constant $Z(\lambda, \nu)$ presents some challenges in the
practical use of CMP models and has been a topic of interest in the CMP literature. In general, there is no simple closed form expression for the series. Furthermore, the value of $Z(\lambda, \nu)$ can change by many orders of magnitude when $\lambda$ and $\nu$ are varied. (TBD: maybe give an overview here or earlier in the intro, and specifics later if they are needed). \citet{ShmueliEtAl2005} give the approximation
%
\begin{align}
Z(\lambda, \nu) &= \frac{ \exp(\nu \lambda^{1/\nu}) }{ \lambda^{(\nu-1)/2\nu} (2\pi)^{(\nu-1)/2} \nu^{1/2} }
\left\{ 1 + O(\lambda^{-1/\nu}) \right\},
\label{eqn:approx}
\end{align}
%
which has been refined further in subsequent literature mentioned in Section~\ref{sec:intro}. The first term in \eqref{eqn:approx} emphasizes that the magnitude of $Z(\lambda, \nu)$ explodes when $\lambda > 1$ as $\nu \rightarrow 0$. For example, $Z(2, 0.075) \approx e^{780.515}$ is too large to store as a double-precision
floating point number, and may evaluate to infinity if care is not taken. In contrast, $Z(\lambda, \nu) \rightarrow 1/(1 - \lambda)$ when $\lambda < 1$ and $\nu \rightarrow 0$.

In practice, the \code{COMPoissonReg} packages does not place constraints on $\lambda$ and $\nu$, except to ensure that they are positive, so that their values are driven by the data and/or user's selection. A hybrid strategy motivated by \eqref{eqn:approx} is taken by \pkg{COMPoissonReg}. Given a small tolerance $\delta > 0$, if $\lambda^{-1/\nu} < \delta$, the first term of \eqref{eqn:approx} dominates the second term, and we take
%
\begin{align}
Z(\lambda, \nu) &\approx \frac{ \exp(\nu \lambda^{1/\nu}) }{ \lambda^{(\nu-1)/2\nu} (2\pi)^{(\nu-1)/2} \nu^{1/2} } \nonumber \\
&=\exp\left\{
\nu \lambda^{1/\nu} - \frac{\nu-1}{2\nu} \log \lambda - \frac{\nu-1}{2} \log(2\pi) - \frac{1}{2} \log \nu
\right\}.
\label{eqn:z-approx}
\end{align}
%
as an approximation. Otherwise, the series is computed truncating the series to a finite number of terms, which is described next. In either case, computations are kept on the log-scale as much as possible to accommodate numbers with potentially very large magnitudes.

If $\lambda^{-1/\nu} \geq \delta$, we approximate $Z(\lambda, \nu)$ by a finite summation $Z^{(M)}(\lambda, \nu) = \sum_{r=0}^M \lambda^r / (r!)^\nu$. The general
approach for this is described in Appendix~B of \citet{ShmueliEtAl2005}.
Here we consider Stirling's approximation in particular; for an adequately
large $M$, Stirling's approximation can be applied to obtain bounds for the
remainder with a closed form. \citet[Section~2.9]{Feller1968} gives the bound
%
\begin{align*}
\sqrt{2\pi} n^{n + 1/2} e^{-n} e^{1 / (12n + 1)} < n! <
\sqrt{2\pi} n^{n + 1/2} e^{-n} e^{1 / (12n)}.
\end{align*}
%
From here, we can obtain bounds which will be convenient in the following calculations.%
\footnote{This form is given at
\url{https://en.wikipedia.org/wiki/Stirling\%27s_approximation}.}
%
\begin{align*}
\sqrt{2\pi} n^{n + 1/2} e^{-n} \leq n! \leq e n^{n + 1/2} e^{-n}
\end{align*}
%
by noting that $e^{1 / (12n + 1)} \geq 1$ for $n \geq 1$ and
$\sqrt{2\pi} e^{1 / (12n)} \leq e$ for $n \geq 2$.
%
We may then bound the truncation error for $Z^{(M)}(\lambda, \nu)$ using
%
\begin{align}
\lvert Z(\lambda, \nu) - Z^{(M)}(\lambda, \nu) \rvert &= Z(\lambda, \nu) - Z^{(M)}(\lambda, \nu) \nonumber \\
&= \sum_{r=M+1}^\infty \frac{\lambda^r}{(r!)^\nu} \nonumber \\
&\leq \sum_{r=M+1}^\infty \frac{\lambda^r}{(2\pi)^{\nu/2} r^{\nu r + \nu/2} e^{-r \nu}} \nonumber \\
&\leq \sum_{r=M+1}^\infty \frac{\lambda^r}{(2\pi)^{\nu/2} (M+1)^{\nu r + \nu/2} e^{-r \nu}} \nonumber \\
&= (2\pi)^{-\nu/2} (M+1)^{-\nu/2} \sum_{r=M+1}^\infty \left( \frac{\lambda e^{\nu}}{(M+1)^{\nu}} \right)^r \label{eqn:geom} \\
&= (2\pi)^{-\nu/2} (M+1)^{-\nu/2} \sum_{r=0}^\infty \left( \frac{\lambda e^{\nu}}{(M+1)^{\nu}} \right)^{r+M+1} \nonumber \\
&= (2\pi)^{-\nu/2} (M+1)^{-\nu/2} \left( \frac{\lambda e^{\nu}}{(M+1)^{\nu}} \right)^{M+1} \frac{1}{1 - \frac{\lambda e^{\nu}}{(M+1)^{\nu}}} \nonumber \\
&=: \Delta_M, \nonumber
\end{align}
%
assuming that $|\lambda e^\nu / (M+1)^\nu| < 1$ for convergence of the
geometric series in \eqref{eqn:geom}. To ensure this we choose $M$ at least
large enough so that
%
\begin{align*}
\lambda e^\nu / (M+1)^\nu < 1 \iff
M > \lambda^{1/\nu} e - 1.
\end{align*}
%
Consider bounding the relative error by a small given number $\epsilon > 0$:
%
\begin{align*}
&\frac{\lvert Z(\lambda, \nu) - Z^{(M)}(\lambda, \nu) \rvert}{Z^{(M)}(\lambda, \nu)}
\leq \frac{\Delta_M}{Z^{(M)}(\lambda, \nu)}
< \epsilon.
\end{align*}
%
This condition can be expressed on the log-scale using
%
\begin{align}
\log \Delta_M - \log Z^{(M)}(\lambda, \nu) < \log \epsilon,
\label{eqn:adaptive-log-scale}
\end{align}
%
where
%
\begin{align*}
\log \Delta_M
= -\frac{\nu}{2} \log(2\pi)
- \nu \left( M+\frac{3}{2} \right) \log (M+1)
+ (M+1) (\nu + \log \lambda)
- \log\left(1 - \frac{\lambda e^\nu}{(M+1)^\nu} \right).
\end{align*}
%
Therefore, we compute $Z^{(M)}(\lambda, \nu)$ until at least
$M > \lambda^{1/\nu} e - 1$, increasing $M$ and updating
$Z^{(M)}(\lambda, \nu)$ until \eqref{eqn:adaptive-log-scale} is satisfied.

The individual terms $\lambda^r / (r!)^\nu$ in the summation may be too large to store at their original scale. Therefore, summation is carried out at the log-scale, wherever possible, using the identity
%
\begin{align*}
\log(x + y) = \log x + \log(1 + \exp\{ \log y - \log x \});
\end{align*}
%
this is especially helpful when $0 < y << x$, as $\log x$ may be kept on the log-scale by the first term of the RHS, and the standard library function \code{log1p} may be used with the second term to accurately compute $\log(1 + \epsilon)$ for very small $\epsilon > 0$.


\paragraph{TBD}
\begin{itemize*}
\item A good way to motivate adding these complications is to show some plots of $f(y) = \lambda^y / (y!)^\nu$.
\item We could potentially compute score and FIM via the Lindsey and Merch trick and avoid numerical derivatives. Can we use our bound above to help guide the truncation there? (The approximation method might not be straightforward to connect). Do we want to deal with this now?
\item We need to explain / demonstrate ymax and the other options somewhere around here.
\end{itemize*}

\subsection{Computation of Functions involving $Z$}
Derivatives of $\log Z(\lambda, \nu)$ are needed to compute expected values, score functions, and covariance matrices needed by \pkg{COMPoissonReg}. Using the hybrid procedure described at the beginning of Section~\ref{sec:normconst} to compute $\log Z(\lambda, \nu)$, we make use of the numerical differentiation in the following ways.

The gradient of the log-likelihood \eqref{eqn:likelihood} is needed in \code{optim} during likelihood maximization.  The Hessian of the log-likelihood is used to estimate $\Var(\hat{\btheta})$ given a solution $\hat{\btheta} = (\vec{\beta}, \vec{\gamma}, \vec{\zeta})$ from \code{optim}. The expected value $\E(Y) = \lambda \frac{\partial}{\partial \lambda} \log Z(\lambda, \nu)$ and variance $\Var(Y) = \text{TBD}$ of $Y \sim \text{CMP}(\lambda, \nu)$ may be of interest to package users as well.

Suppose $Y \sim \text{CMP}(\lambda, \nu)$. We use numerical differentiation to compute the expected value
%
\begin{align}
\E(Y) &= \lambda \frac{\partial}{\partial \lambda} \log Z(\lambda, \nu), \nonumber \\
&= \lim_{\epsilon \downarrow 0} \lambda \frac{\log Z(\lambda + \epsilon, \nu) - \log Z(\lambda, \nu)}{\epsilon} \nonumber \\
&\approx \frac{\log Z(\lambda + h, \nu) - \log Z(\lambda, \nu) }{h / \lambda},
\label{eqn:fwdderiv}
\end{align}
%
given a small $h > 0$. Forward differentiation respects the boundary $\lambda = 0$.

 SimilarThe gradient and Hessian are computed 

The gradient and Hessian of the log-likelihood \eqref{} involve first and second derivatives of $\log Z(\lambda, \nu)$. 


How are they used?

The gradient and Hessian of $\log Z(\lambda, \nu)$ are also needed to compute the Fisher Information Matrix for the CMP and ZICMP models; see the expressions in \citet{SellersRaim2016}. We may compute them in a similar manner as \eqref{eqn:fwdderiv}. For a given function $g : \mathbb{R}^k \rightarrow \mathbb{R}$, we may compute the $j$th entry of the gradient as
%
\begin{align*}
\nabla_j g(\vec{x})
\approx \frac{ g(\vec{x} + h \vec{e}_j) - g(\vec{x}) }{h},
\end{align*}
%
and the $(j,l)$th entry of the Hessian as
%
\begin{align*}
\nabla^2_{jl} g(\vec{x})
\approx \frac{g(\vec{x} + h \vec{e}_j + h \vec{e}_l) -
g(\vec{x} + h \vec{e}_j) -
g(\vec{x} + h \vec{e}_l) +
g(\vec{x})}{h^2}.
\end{align*}
%
Here, $\vec{e}_1, \ldots, \vec{e}_k$ represent the columns of a $k \times k$ identity matrix.

We make use of various derivatives of $\log Z(\lambda, \nu)$ as well, especially to compute the Fisher Information matrix \citep{SellersRaim2016}. These derivatives can be obtained as closed-form infinite sums, but a very large number of terms may be needed to get a reasonable approximation. (TBD: The issue appears potentially worse in some of the derivatives than in $\log Z(\lambda, \nu)$, but I did not investigate this in detail. Also, we no long use this part of the code, and just use the Hessian instead of the FIM).


\begin{comment}
\section{Delete This}
To include a PDF vignette that is compiled from a plain LaTeX file,
all you need is the LaTeX file with LaTeX comments containing
meta directives to R such that it will be listed as a vignette in the
package.
The LaTeX-based vignette file should be placed in the
\code{vignettes/} directory of your package.  If your LaTeX file
includes other files such as figure files, these should also be
located in the \file{vignettes/}.
For instance, this PDF document was compiled from LaTeX file:
%
\begin{enumerate}
\item \code{vignettes/R\_packages-LaTeX\_vignettes.ltx}
\end{enumerate}
%
which contains the following meta directives at the top of the file:
%
\begin{CodeInput}[numbers=left,xleftmargin=5mm]
%\VignetteIndexEntry{R packages: LaTeX vignettes}
%\VignetteEngine{R.rsp::tex}
%\VignetteKeyword{R}
%\VignetteKeyword{package}
%\VignetteKeyword{vignette}
%\VignetteKeyword{LaTeX}
\end{CodeInput}
%
Here I choose filename extension \code{*.ltx}, which is a lesser known LaTeX
filename extension, because if one uses \code{*.tex}, then \code{R CMD check}
will give a NOTE complaining that the file is a stray file that should not be
part of the built package.

Also, the above first two entries are required whereas the keyword
entries are optional.  Note also that the
%\code{\%{\bs}VignetteIndexEntry\{\}} controls the title shown in R's
help indices as well as in online package respositories such as CRAN. 

As for any type of (non-Sweave) package vignette, don't forget to specify:
%
\begin{CodeInput}
Suggests: R.rsp
VignetteBuilder: R.rsp
\end{CodeInput}
%
in your package's DESCRIPTION file.  That's all it takes to include a
LaTeX file that will be compiled into a PDF vignette as part of the
package build.
\end{comment}

\bibliographystyle{plainnat}
\bibliography{references}
%\input{references.bbl}













\appendix

\clearpage


\section{Adaptive Computation of $Z$ (TBD: Obsolete)}
Let $h(x) := x \log \lambda - \nu \log \Gamma(x+1)$ denote the unnormalized CMP log-density and $\delta > 0$ be a small prespecified number. In this section, we will discuss a procedure to find an $M$ such that 
%
\begin{align}
\exp\{ h(x) \} < \delta, \quad \text{for all $x \geq M$}.
\label{eqn:diff}
\end{align}
%
To do this, we will establish that $h(x)$ is a concave function of $x \in \mathbb{R}$. We have first and second derivatives
%
\begin{align*}
&h'(x) = \log \lambda - \nu \frac{\partial}{\partial x} \log \Gamma(x+1)
= \log \lambda - \nu \psi(x+1), \\
&h''(x) = - \nu \frac{\partial^2}{\partial x^2} \log \Gamma(x+1).
\end{align*}
%
where $\psi(x) = \frac{\partial}{\partial x} \log \Gamma(x)$ is the digamma function. It is well-known that $\log \Gamma(\cdot)$ is a convex function (TBD: cite?), so $h(x)$ has a negative second derivative and is therefore a concave function of $x$. Then $h(x)$ has a unique maximum $x^*$ such that $h'(x) = 0$ Therefore, $h'(x)$ will be positive for $x < x^*$, zero for $x = x^*$, and negative for $x > x^*$. Consequently, $h(x)$ will be decreasing for $x > x^*$ so that \eqref{eqn:diff} can be satisfied.

Our strategy will be to compute $Z^{(M)}(\lambda, \nu)$ in three phases: first, increase $M$ until $h'(M) \leq 0$ is satisfied; second, increase $M$ until $h(M) < \log \delta$ is satisfied; finally, increase $M$ until \eqref{eqn:adaptive-log-scale} is satisfied. To facilitate computing $h'(x)$, we recall the well-known property that $\psi(x+1) = \psi(x) + \frac{1}{x}$, so that
%
\begin{align*}
\psi(M+1) = \psi(M) + \frac{1}{M} = \cdots = \psi(1) + \sum_{r=1}^M \frac{1}{r},
\end{align*}
%
where $-\psi(1) = \gamma \approx 0.5772157$ is the Euler-Mascheroni constant. We now state the adaptive computation as Algorithm~\ref{alg:adaptz}.

\begin{algorithm}
\caption{An adaptive computation of the series in $Z(\lambda, \nu)$.}
\label{alg:adaptz}
\begin{algorithmic}
\Function{AdaptiveZ}{$\lambda, \nu, \delta, \epsilon$}
\State $z \leftarrow 0$,
$h(y) \leftarrow \infty$,
$\psi \leftarrow -\gamma$,
$y \leftarrow 0$,
$h'(y) \leftarrow \infty$
\While{$h'(y) > 0$}
\State $h(y) \leftarrow y \log \lambda - \nu \log \Gamma(y+1)$
\State $z \leftarrow z + \exp\{h(y)\}$
\State $\psi \leftarrow \psi + 1 / (y+1)$
\State $h'(y) \leftarrow \log \lambda - \nu \psi$
\State $y \leftarrow y + 1$
\EndWhile

\While{$h(y) > \log \delta$}
\State $h(y) \leftarrow y \log \lambda - \nu \log \Gamma(y+1)$
\State $z \leftarrow z + \exp\{ h(y) \}$
\State $y \leftarrow y + 1$
\EndWhile

\While{$M \leq \lambda^{1/\nu} e - 1$}
\State $h(y) \leftarrow y \log \lambda - \nu \log \Gamma(y+1)$
\State $z \leftarrow z + \exp\{ h(y) \}$
\State $y \leftarrow y + 1$
\EndWhile

\State Compute $\log \Delta_M$ according to Section~\ref{sec:quantify-error}
\While{$\log \Delta_M - \log z \geq \log \epsilon$}
\State $h(y) \leftarrow y \log \lambda - \nu \log \Gamma(y+1)$
\State $z \leftarrow z + \exp\{ h(y) \}$
\State $y \leftarrow y + 1$
\State Recompute $\log \Delta_M$
\EndWhile

\State \Return $z$
\EndFunction
\end{algorithmic}
\end{algorithm}
%
To address issue A3, we can employ a ``hybrid'' method to compute $\log Z(\lambda, \nu)$. Given some threshold $\kappa > 0$, we return $\log \tilde{Z}(\lambda, \nu)$ computed via \eqref{eqn:z-approx} if $\lambda^{-1/\nu} < \kappa$, and $\log \tilde{Z}(\lambda, \nu)$ computed via Algorithm~\ref{alg:adaptz} otherwise.

\end{document}

