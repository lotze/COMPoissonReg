---
title: "A Vignette for the COMPoissonReg Package"
author: Andrew and ?
#output:
#  html_vignette:
#    number_sections: true
#    extra_dependencies: ["common"]
output: 
  pdf_document:
    highlight: default
    number_sections: true
    toc: true
    extra_dependencies:
      common: null
      # hyperref: ["colorlinks=true", "citecolor=blue", "linkcolor=blue", "urlcolor=blue"]
      # hyperref: ["linkcolor=blue"]
date: "COMPoissonReg v0.8.0"
bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{A Vignette for the COMPoissonReg Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  prompt = TRUE,
  comment = ""
)
```

```{r setup, include = FALSE}
library(COMPoissonReg)
set.seed(1234)
```

# Introduction
\label{sec:intro}

TBD:

- See the manual for a full specification of the public interface.

# Conway Maxwell Poisson Distribution
\label{sec:cmp}

Let $Y \sim \text{CMP}(\lambda, \nu)$ be a Conway Maxwell Poisson (CMP) random variable with density
<!-- -->
\begin{align*}
f(y \mid \lambda, \nu) = \frac{\lambda^y}{(y!)^\nu Z(\lambda, \nu)}, \quad y \in \mathbb{N}, \quad
Z(\lambda, \nu) = \sum_{r=0}^\infty \frac{\lambda^r}{(r!)^\nu},
\end{align*}
<!-- -->
where $\lambda > 0$, $\nu > 0$, and $\mathbb{N}$ represents the nonnegative integers $\{ 0, 1, 2, \ldots\}$. Three notable special cases of $\text{CMP}(\lambda, \nu)$ help to demonstrate its flexibility in capturing dispersion.

a. The case $\nu = 1$ corresponds to $\text{Poisson}(\lambda)$. 

a. When $\lambda \in (0,1)$ and $\nu \rightarrow 0$, the $\text{CMP}(\lambda, \nu)$ distribution becomes $\text{Geometric}(\lambda)$ with density $f(y \mid \lambda) = \lambda^y (1 - \lambda)^{1-y}$ for $y \in \mathbb{N}$, which is overdispersed relative to Poisson. 

a. When $\nu \rightarrow \infty$, $\text{CMP}(\lambda, \nu)$ converges to a $\text{Bernoulli}(\lambda / (1 + \lambda))$ distribution which is underdispersed relative to Poisson.

@ShmueliEtAl2005 discusses some important properties of CMP as a count distribution, and @SellersShmueli2010 demonstrate the use of CMP in regression modeling.

## Normalizing Constant
The normalizing constant $Z(\lambda, \nu)$ presents some challenges in the practical use of CMP models and has been a topic of interest in the CMP literature. In general, there is no simple closed form expression for the series $Z(\lambda, \nu)$. Furthermore, its value can change by many orders of magnitude when $\lambda$ and $\nu$ are varied. (TBD: maybe give an overview here or earlier in the intro, and specifics later if they are needed). @ShmueliEtAl2005 give the approximation
<!-- -->
\begin{align}
Z(\lambda, \nu) &= \frac{ \exp(\nu \lambda^{1/\nu}) }{ \lambda^{(\nu-1)/2\nu} (2\pi)^{(\nu-1)/2} \nu^{1/2} }
\left\{ 1 + O(\lambda^{-1/\nu}) \right\},
\label{eqn:approx}
\end{align}
<!-- -->
which has been refined further in subsequent literature mentioned in Section \ref{sec:intro}. The first term in \eqref{eqn:approx} emphasizes that the magnitude of $Z(\lambda, \nu)$ explodes when $\lambda > 1$ as $\nu \rightarrow 0$. For example, $Z(2, 0.075) \approx e^{780.515}$ is too large to store as a double-precision floating point number, and may evaluate to infinity if care is not taken. In contrast, $Z(\lambda, \nu) \rightarrow 1/(1 - \lambda)$ when $\lambda < 1$ and $\nu \rightarrow 0$.

In practice, the `COMPoissonReg` package does not place constraints on $\lambda$ and $\nu$, except to ensure that they are positive, so that their values are driven by the data or the user's selection. A hybrid strategy motivated by \eqref{eqn:approx} is taken by `COMPoissonReg`. Given a small tolerance $\delta > 0$, if $\lambda^{-1/\nu} < \delta$, the first term of \eqref{eqn:approx} dominates the second term, and we take
<!-- -->
\begin{align}
Z(\lambda, \nu) &\approx \frac{ \exp(\nu \lambda^{1/\nu}) }{ \lambda^{(\nu-1)/2\nu} (2\pi)^{(\nu-1)/2} \nu^{1/2} } \nonumber \\
&=\exp\left\{
\nu \lambda^{1/\nu} - \frac{\nu-1}{2\nu} \log \lambda - \frac{\nu-1}{2} \log(2\pi) - \frac{1}{2} \log \nu
\right\}.
\label{eqn:z-approx}
\end{align}
<!-- -->
as an approximation. Otherwise, the series is computed truncating the series to a finite number of terms, which is described next. In either case, computations are kept on the log-scale as much as possible to accommodate numbers with potentially very large magnitudes.

We approximate $Z(\lambda, \nu)$ by a finite summation $Z^{(M)}(\lambda, \nu) = \sum_{r=0}^M \lambda^r / (r!)^\nu$ if
<!-- --->
\begin{align}
\lambda^{-1/\nu} \geq \delta.
\label{eqn:can_trunc}
\end{align}
<!-- --->
The general approach for this is described in Appendix B of @ShmueliEtAl2005. Here we consider Stirling's approximation in particular; for an adequately
large $M$, Stirling's approximation can be applied to obtain bounds for the remainder with a closed form. @Feller1968 [, Section 2.9] gives the bound
<!-- -->
\begin{align*}
\sqrt{2\pi} n^{n + 1/2} e^{-n} e^{1 / (12n + 1)} < n! <
\sqrt{2\pi} n^{n + 1/2} e^{-n} e^{1 / (12n)}.
\end{align*}
<!-- -->
(This form is stated at <https://en.wikipedia.org/wiki/Stirling\%27s_approximation>.) Now we can obtain bounds which will be convenient in the following calculations.
<!-- -->
\begin{align*}
\sqrt{2\pi} n^{n + 1/2} e^{-n} \leq n! \leq e n^{n + 1/2} e^{-n}
\end{align*}
<!-- -->
by noting that $e^{1 / (12n + 1)} \geq 1$ for $n \geq 1$ and $\sqrt{2\pi} e^{1 / (12n)} \leq e$ for $n \geq 2$. We may then bound the truncation error for $Z^{(M)}(\lambda, \nu)$ using
<!-- -->
\begin{align}
\lvert Z(\lambda, \nu) - Z^{(M)}(\lambda, \nu) \rvert &= Z(\lambda, \nu) - Z^{(M)}(\lambda, \nu) \nonumber \\
&= \sum_{r=M+1}^\infty \frac{\lambda^r}{(r!)^\nu} \nonumber \\
&\leq \sum_{r=M+1}^\infty \frac{\lambda^r}{(2\pi)^{\nu/2} r^{\nu r + \nu/2} e^{-r \nu}} \nonumber \\
&\leq \sum_{r=M+1}^\infty \frac{\lambda^r}{(2\pi)^{\nu/2} (M+1)^{\nu r + \nu/2} e^{-r \nu}} \nonumber \\
&= (2\pi)^{-\nu/2} (M+1)^{-\nu/2} \sum_{r=M+1}^\infty \left( \frac{\lambda e^{\nu}}{(M+1)^{\nu}} \right)^r \label{eqn:geom} \\
&= (2\pi)^{-\nu/2} (M+1)^{-\nu/2} \sum_{r=0}^\infty \left( \frac{\lambda e^{\nu}}{(M+1)^{\nu}} \right)^{r+M+1} \nonumber \\
&= (2\pi)^{-\nu/2} (M+1)^{-\nu/2} \left( \frac{\lambda e^{\nu}}{(M+1)^{\nu}} \right)^{M+1} \frac{1}{1 - \frac{\lambda e^{\nu}}{(M+1)^{\nu}}} \nonumber \\
&=: \Delta_M, \nonumber
\end{align}
<!-- -->
assuming that $|\lambda e^\nu / (M+1)^\nu| < 1$ for convergence of the
geometric series in \eqref{eqn:geom}. To ensure this we choose $M$ at least
large enough so that
<!-- -->
\begin{align*}
\lambda e^\nu / (M+1)^\nu < 1 \iff
M > \lambda^{1/\nu} e - 1.
\end{align*}
<!-- -->
Consider bounding the relative error by a small given number $\epsilon > 0$:
<!-- -->
\begin{align*}
&\frac{\lvert Z(\lambda, \nu) - Z^{(M)}(\lambda, \nu) \rvert}{Z^{(M)}(\lambda, \nu)}
\leq \frac{\Delta_M}{Z^{(M)}(\lambda, \nu)}
< \epsilon.
\end{align*}
<!-- -->
This condition can be expressed on the log-scale using
<!-- -->
\begin{align}
\log \Delta_M - \log Z^{(M)}(\lambda, \nu) < \log \epsilon,
\label{eqn:adaptive-log-scale}
\end{align}
<!-- -->
where
<!-- -->
\begin{align*}
\log \Delta_M
= -\frac{\nu}{2} \log(2\pi)
- \nu \left( M+\frac{3}{2} \right) \log (M+1)
+ (M+1) (\nu + \log \lambda)
- \log\left(1 - \frac{\lambda e^\nu}{(M+1)^\nu} \right).
\end{align*}
<!-- -->
Therefore, we compute $Z^{(M)}(\lambda, \nu)$ until at least
$M > \lambda^{1/\nu} e - 1$, increasing $M$ and updating
$Z^{(M)}(\lambda, \nu)$ until \eqref{eqn:adaptive-log-scale} is satisfied.

The individual terms $\lambda^r / (r!)^\nu$ in the summation may be too large to store at their original scale. Therefore, summation is carried out at the log-scale, wherever possible, using the identity
<!-- -->
\begin{align}
\log(x + y) = \log x + \log(1 + \exp\{ \log y - \log x \});
\label{eqn:logadd}
\end{align}
<!-- -->
this is especially helpful when $0 < y << x$, as $\log x$ may be kept on the log-scale by the first term of the right-hand side of \eqref{eqn:logadd}, and the standard library function `log1p` may be used with the second term to accurately compute $\log(1 + \epsilon)$ for very small $\epsilon > 0$.


The tolerances $\delta$ and $\epsilon$ are specified in `COMPoissonReg` using global options `COMPoissonReg.hybrid.tol` and `COMPoissonReg.truncate.tol` respectively. Furthermore, the maximum value of $M$ is specified by the option `COMPoissonReg.ymax`. This is a safety measure that prevents very large computations unless the user opts to allow them. All options can be accessed using R's `getOption` function, with default values shown.

```{r}
getOption("COMPoissonReg.hybrid.tol")
getOption("COMPoissonReg.truncate.tol")
getOption("COMPoissonReg.ymax")
```

The `ncmp` function computes the $Z(\lambda, nu)$ and returns its value either on its original scale or the log-scale. The `method` argument specifies how the computation will be carried out. The argument `method = "approx"` corresponds to \ref{eqn:z-approx}, while `method = "trunc"` uses the truncated sum $Z^{(M)}(\lambda, \nu)$, and the default `method = "hybrid"` uses the hybrid strategy.

```{r}
ncmp(lambda = 1.5, nu = 1.2)
ncmp(lambda = 1.5, nu = 1.2, log = TRUE)
ncmp(lambda = 1.5, nu = 1.2, log = TRUE, method = "approx")
ncmp(lambda = 1.5, nu = 1.2, log = TRUE, method = "trunc")
```

The function `tcmp` returns the truncation value $M$ obtained from `ncmp` with `method = "trunc"`.

```{r}
suppressWarnings(
	tcmp(lambda = 1.5, nu = c(1, 0.5, 0.2, 0.1, 0.05, 0.03))
)
```

Note that `tcmp` returns `1e6` for the last `nu` value of ``0.03`` because we have reached the maximum value specified by `COMPoissonReg.ymax = 1e6` before capturing a sufficient proportion of the normalizing constant using \eqref{eqn:adaptive-log-scale}. A warning message about this is also output, but has been suppressed from the vignette. Here, support values with with non-negligible mass may be left out. Therefore, let us increase `COMPoissonReg.ymax` for the current session using R's `options` function and try again.

```{r}
options(COMPoissonReg.ymax = 3e6)
tcmp(lambda = 1.5, nu = c(1, 0.5, 0.2, 0.1, 0.05, 0.03))
```

Now that we have a somewhat robust computation for the normalizing constant, let us plot this interesting behavior when $\lambda > 1$ and $\nu$ decreases.

```{r, fig.width = 5, fig.height = 3, fig.align = "center", prompt = FALSE}
library(ggplot2)

nu_seq = seq(0.03, 1.5, length.out = 20)
norm1 = ncmp(lambda = 0.5, nu = nu_seq, log = TRUE)
norm2 = ncmp(lambda = 1.05, nu = nu_seq, log = TRUE)
norm3 = ncmp(lambda = 1.20, nu = nu_seq, log = TRUE)

ggplot() +
	geom_point(data = data.frame(x = nu_seq, y = norm1), aes(x = x, y = y), pch = 1) +
	geom_point(data = data.frame(x = nu_seq, y = norm2), aes(x = x, y = y), pch = 2) +
	geom_point(data = data.frame(x = nu_seq, y = norm3), aes(x = x, y = y), pch = 3) +
	xlab("nu") +
	ylab("log of normalizing constant") +
	theme_bw()
```
We see that `lambda = 1.2` (represented by $+$ symbols) reaches a value of $Z(\lambda, \nu) \approx e^{18.67}$ when `nu = 0.03`.

## Density, Generation, CDF, and Quantile Functions
\label{sec:cmp-dist}

The respective functions for CMP density, variate generation, CDF, and quantile functions are `dcmp`, `rcmp`, `pcmp` , and `qcmp`. Their usage is similar to distribution functions provided by the R `stats` package.

```{r}
dcmp(0, lambda = 10, nu = 0.9)
dcmp(0:17, lambda = 10, nu = 0.9, log = TRUE)
dcmp(c(0, 1, 2), lambda = c(10, 11, 12), nu = c(0.9, 1.0, 1.1), log = TRUE)
```

```{r}
rcmp(50, lambda = 10, nu = 0.9)
```

```{r}
pcmp(0:17, lambda = 10, nu = 0.9)
```

```{r}
qq = seq(0, 1, length.out = 10)
qcmp(qq, lambda = 10, nu = 0.9)
```

As a quick test, let us ensure that the empirical density values, cumulative probabilities, and quantiles line up with the ones computed via `dcmp`, `pcmp`, and `qcmp`.

```{r, fig.width = 3, fig.height = 3, fig.align = "center", prompt = FALSE, fig.show = "hold"}
library(ggplot2)

n = 100000
lambda = 0.5
nu = 0.1
x = rcmp(n, lambda, nu)

xx = seq(0, max(x))
qq = seq(0, 1, length.out = 100)

fx = dcmp(xx, lambda, nu)
px = pcmp(xx, lambda, nu)
qx = qcmp(qq, lambda, nu)

qx_emp = quantile(x, probs = qq)

ggplot() +
	geom_bar(data = data.frame(x = x), aes(x = x, y = ..prop..), fill = "NA",
		col = "black") +
	geom_point(data = data.frame(x = xx, fx = fx), aes(x, fx)) +
	ylab("Proportion") +
	ggtitle("Empirical Density vs. dcmp") +
	theme_bw()

ggplot() +
	stat_ecdf(data = data.frame(x = x), aes(x), geom = "step") +
	geom_point(data = data.frame(x = xx, px = px), aes(x, px)) +
	ylab("Proportion") +
	ggtitle("Empirical CDF vs. pcmp") +
	theme_bw()

ggplot() +
	geom_point(data = data.frame(x = qq, qx_emp = qx_emp), aes(qq, qx_emp), pch = 1) +
	geom_point(data = data.frame(x = qq, qx = qx), aes(qq, qx), pch = 3) +
	xlab("Probability") +
	ylab("Quantile") +
	ggtitle("Empirical Quantiles vs. qcmp") +
	theme_bw()
```

(TBD: Why does the 1.0 quantile seem to be off?)

## Expected Value and Variance
\label{sec:cmp-ev}

For $Y \sim \text{CMP}(\lambda, \nu)$, we can consider computing the expectation and variance of $Y$ in two ways. First, if there is a moderately-sized $M$ where $\{ 0, \ldots, M \}$ contains all but a negligible amount of the mass of $Y$, we can compute the moments using
<!-- -->
\begin{align*}
\E(Y) = \sum_{y=0}^M y \frac{\lambda^y}{(y!)^\nu Z(\lambda, \nu)}, \quad
\E(Y^2) = \sum_{y=0}^M y^2 \frac{\lambda^y}{(y!)^\nu Z(\lambda, \nu)}, \quad
\Var(Y) = \E(Y^2) - [\E(Y)]^2.
\end{align*}
<!-- -->
Otherwise, a different approach is taken. Notice that the expected value is related to the first derivative of the log-normalizing constant via
<!-- -->
\begin{align*}
&\frac{\partial}{\partial \lambda} \log Z(\lambda, \nu)
= \frac{ \frac{\partial}{\partial \lambda} Z(\lambda, \nu) }{ Z(\lambda, \nu) }
= \frac{1}{Z(\lambda, \nu)} \sum_{y=0}^\infty y \frac{\lambda^{y-1}}{(y!)^\nu} \\
&\iff \E(Y) = \lambda \frac{\partial}{\partial \lambda} \log Z(\lambda, \nu).
\end{align*}
<!-- -->
Also
<!-- -->
\begin{align*}
&\frac{\partial^2}{\partial \lambda^2} \log Z(\lambda, \nu)
= \frac{ Z(\lambda, \nu) \frac{\partial^2}{\partial \lambda^2} Z(\lambda, \nu) - [\frac{\partial}{\partial \lambda} Z(\lambda, \nu)]^2}{ Z(\lambda, \nu)^2 }
= \frac{1}{Z(\lambda, \nu)} \sum_{y=0}^\infty y(y-1) \frac{\lambda^{y-2}}{(y!)^\nu} - \left[ \frac{\E(Y)}{\lambda} \right]^2 \\
&\iff \lambda^2 \frac{\partial^2}{\partial \lambda^2} \log Z(\lambda, \nu) = \E[Y(Y-1)] - [\E(Y)]^2 = \Var(Y) - \E(Y) \\
&\iff \Var(Y) = \lambda^2 \frac{\partial^2}{\partial \lambda^2} \log Z(\lambda, \nu) + \E(Y).
\end{align*}
<!-- -->
Therefore, we may use first and second derivatives of $\log Z(\lambda, \nu)$, with respect to $\lambda$, to compute $\E(Y)$ and $\Var(Y)$. To decide whether to use the truncation or the differentiation approaches, we check condition \eqref{eqn:can_trunc}: if it holds, there is a sufficiently small $M$ to use the truncation method.

The `ecmp` and `vcmp` functions implement this computation of the expectation and variance.

```{r}
ecmp(lambda = 10, nu = 1.2)
ecmp(lambda = 1.5, nu = 0.5)
ecmp(lambda = 1.5, nu = 0.05)
ecmp(lambda = 1.5, nu = 0.05, method = "trunc")
ecmp(lambda = 1.5, nu = 0.05, method = "approx")
```

```{r}
vcmp(lambda = 10, nu = 1.2)
vcmp(lambda = 1.5, nu = 0.5)
vcmp(lambda = 1.5, nu = 0.05)
vcmp(lambda = 1.5, nu = 0.05, method = "trunc")
vcmp(lambda = 1.5, nu = 0.05, method = "approx")
```

As with the `ncmp` function, `method = "approx"` explicitly requests the differentiation approach, `method = "trunc"` requests truncation, and the default `method = "hybrid"` uses condition \eqref{eqn:can_trunc} to decide.

Provided that an enormously large truncation value $M$ is not required, other moments can be computed using `tcmp` by truncated sums.

```{r}
M = tcmp(lambda = 1.5, nu = 0.05)
xx = seq(0, M)

print(M)
sum(xx^3 * dcmp(xx, lambda, nu))    # E(X^3)
sum(xx^4 * dcmp(xx, lambda, nu))    # E(X^4)
```

# Zero-Inflated Conway Maxwell Poisson Distribution

Let $S \sim \text{Bernoulli}(p)$ and $T \sim \text{CMP}(\lambda, \nu)$ be independent random variables. Then $Y = (1-S) T$ has Zero-Inflated Conway Maxwell Poisson distribution $\text{ZICMP}(\lambda, \nu, p)$ with density
<!-- -->
\begin{align*}
f(y \mid \lambda, \nu, p) =  (1 - p) \frac{\lambda^{y}}{(y!)^{\nu} Z(\lambda, \nu)}
+ p \cdot \ind(y = 0), \quad y \in \mathbb{N}.
\end{align*}
<!-- -->
Like CMP, several interesting special cases are obtained by varying $\nu$. 

a. Taking $\nu = 1$ corresponds to Zero-Inflated Poisson $\text{ZIP}(\lambda, p)$ with density $f(y \mid \lambda, p) = (1 - p) e^{-\lambda} \lambda^{y} / y! + p \cdot \ind(y = 0)$.

a. When $\lambda \in (0,1)$ and $\nu \rightarrow 0$, $\text{ZICMP}(\lambda, \nu)$ converges to a Zero-Inflated Geometric distribution with density $f(y \mid \lambda, p) = (1 - p) \lambda^y (1 - \lambda)^{1-y} + p \cdot \ind(y = 0)$ for $y \in \mathbb{N}$.

a. Finally, when $\nu \rightarrow \infty$, $\text{ZICMP}(\lambda, \nu, p)$ converges to a ``Zero-Inflated Bernoulli'' distribution with density 
<!-- -->
\begin{math}
f(y \mid \lambda, p) = (1 - p) \left[ \lambda/(1+\lambda) \right]^y \left[ 1/(1+\lambda) \right]^{1-y} + p \cdot \ind(y = 0).
\end{math}

In case (c) the $\lambda$ and $p$ parameters are not identifiable, and users may want to avoid ZICMP in data analyses with extreme underdispersion. @SellersRaim2016 provide more discussion on ZICMP and its use in regression modeling.

## Density, Generation, CDF, and Quantile Functions
The respective functions for ZICMP density, variate generation, CDF, and quantile functions are `dzicmp`, `rzicmp`, `pzicmp` , and `qzicmp`. They make use of the CMP implementation described in Section \ref{sec:cmp}. The relationship between the CMP and ZICMP densities and variate generation mechanisms was given earlier in the present section. Furthermore, let $F(x \mid \lambda, \nu)$ and $F^{-}(\phi \mid \lambda, \nu)$ denote the CDF and quantile function of $\text{CMP}(\lambda, \nu)$ and $F(x \mid \lambda, \nu, p)$ and $F^{-}(\phi \mid \lambda, \nu, p)$ denote the CDF and quantile function of $\text{ZICMP}(\lambda, \nu, p)$. We have
<!-- -->
\begin{align*}
F(x \mid \lambda, \nu, p) = (1-p) F(x \mid \lambda, \nu) + p \cdot \ind(x \geq 0)
\end{align*}
<!-- -->
and, for a given $\phi \in [0,1]$,
<!-- -->
\begin{align*}
F^{-1}(\phi \mid \lambda, \nu, p)
&= \inf\{ x \in \mathbb{N} : F(x \mid \lambda, \nu, p) \geq \phi \} \\
&= \inf\{ x \in \mathbb{N} : (1-p) F(x \mid \lambda, \nu) + p \cdot \ind(x \geq 0) \geq \phi \} \\ 
&= \inf\{ x \in \mathbb{N} : F(x \mid \lambda, \nu) \geq (\phi - p) / (1 - p) \} \\
&= F^{-1}\left( \frac{\phi - p}{1 - p} \mid \lambda, \nu \right)
\end{align*}


dzicmp, rzicmp, pzicmp, and qzicmp functions

```{r}
print("TBD")
```

As a quick test, let us ensure that the empirical density values, cumulative probabilities, and quantiles line up with the ones computed via `dzicmp`, `pzicmp`, and `qzicmp`.

```{r, fig.width = 3, fig.height = 3, fig.align = "center", prompt = FALSE, fig.show = "hold"}
library(ggplot2)

n = 100000
lambda = 0.5
nu = 0.1
p = 0.5
x = rzicmp(n, lambda, nu, p)

xx = seq(0, max(x))
qq = seq(0, 1, length.out = 100)

fx = dzicmp(xx, lambda, nu, p)
px = pzicmp(xx, lambda, nu, p)
qx = qzicmp(qq, lambda, nu, p)

qx_emp = quantile(x, probs = qq)

ggplot() +
	geom_bar(data = data.frame(x = x), aes(x = x, y = ..prop..), fill = "NA",
		col = "black") +
	geom_point(data = data.frame(x = xx, fx = fx), aes(x, fx)) +
	ylab("Proportion") +
	ggtitle("Empirical Density vs. dzicmp") +
	theme_bw()

ggplot() +
	stat_ecdf(data = data.frame(x = x), aes(x), geom = "step") +
	geom_point(data = data.frame(x = xx, px = px), aes(x, px)) +
	ylab("Proportion") +
	ggtitle("Empirical CDF vs. pzicmp") +
	theme_bw()

ggplot() +
	geom_point(data = data.frame(x = qq, qx_emp = qx_emp), aes(qq, qx_emp), pch = 1) +
	geom_point(data = data.frame(x = qq, qx = qx), aes(qq, qx), pch = 3) +
	xlab("Probability") +
	ylab("Quantile") +
	ggtitle("Empirical Quantiles vs. qzicmp") +
	theme_bw()
```

(TBD: `qzicmp` looks like it might be off.)

## Expectation and Variance
The expected value and variance of $Y \sim \text{ZICMP}(\lambda, \nu, p)$ are apparent from the construction $Y = (1-S) T$ given earlier in this section. Namely,
<!-- -->
\begin{align*}
\E(Y) = (1-p) \E(T)
\quad \text{and} \quad
\Var(Y) = (1-p) \left\{ \Var(T) + p[\E(T)]^2 \right\}
\end{align*}
<!-- -->
may be obtained using formulas for iterated conditional expectations and variances. They may be invoked in `COMPoissonReg` using the `ezicmp` and `vzicmp` functions respectively. These make use of the logic described in Section \ref{sec:cmp-ev} to compute $\E(T)$ and $\Var(T)$.

```{r}
ezicmp(lambda = 1.5, nu = 0.5, p = 0.1)
ezicmp(lambda = 1.5, nu = 0.5, p = c(0.1, 0.2, 0.5))
```

```{r}
vzicmp(lambda = 1.5, nu = 0.5, p = 0.1)
vzicmp(lambda = 1.5, nu = 0.5, p = c(0.1, 0.2, 0.5))
```

# Regression Modeling with CMP and ZICMP

Suppose there are $n$ subjects with count-valued outcomes $y_1, \ldots, y_n$ and covariates $\vec{x}_i \in \mathbb{R}^{d_1}$, $\vec{s}_i \in
\mathbb{R}^{d_2}$, and $\vec{w}_i \in \mathbb{R}^{d_3}$ for $i = 1, \ldots, n$. The `COMPoissonReg` package fits Zero-Inflated COM-Poisson regression models of the form
<!-- -->
\begin{align*}
Y_i \indep \text{ZICMP}(\lambda_i, \nu_i, p_i), \quad i = 1, \ldots, n,
\end{align*}
<!-- -->
where $\log \lambda_i = \vec{x}_i^\top \vec{\beta}$, $\log \nu_i = \vec{s}_i^\top \vec{\gamma}$, and $\logit p_i = \vec{w}_i^\top \vec{\zeta}$. Writing $\btheta = (\vec{\beta}, \vec{\gamma}, \vec{\zeta})$, the likelihood is
<!-- -->
\begin{align}
L(\btheta) =
\prod_{i=1}^n \left[(1 - p_i)
\frac{\lambda_i^{y_i}}{(y_i!)^{\nu_i} Z(\lambda_i, \nu_i)}
+ p_i \ind(y_i = 0)
\right].
\label{eqn:likelihood}
\end{align}

TBD: Describe `glm.cmp` here.

## CMP Regression

## ZICMP Regression

# TBD

1. CMP regression
	a. Regression model
	a. Recall freight data, its backstory, and a CMP paper where it was used
	a. Example data analyses with freight data
		- Mention alternatives to pass data to glm.cmp
		- Mention default optimizer and how to change
		- Mention changing default optim.control
		- Try a few model variants: intercepts only and with covariates.
		- Mention use of offsets with glm.cmp. Here we should show a little
		  workflow demonstrating that the offset gets carried through.

1. ZICMP
	a. Regression model
	a. Recall couple data, its backstory, and ZICMP paper where it was used
	a. Example data analyses with couple data
		- Try a few model variants: intercepts only and with covariates.


# References

