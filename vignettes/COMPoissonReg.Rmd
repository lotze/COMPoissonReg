---
title: "A Vignette for the COMPoissonReg Package"
author: Andrew and ?
#output:
#  html_vignette:
#    number_sections: true
#    extra_dependencies: ["common"]
output: 
  pdf_document:
    highlight: default
    number_sections: true
    toc: true
    extra_dependencies:
      common: null
      # hyperref: ["colorlinks=true", "citecolor=blue", "linkcolor=blue", "urlcolor=blue"]
      # hyperref: ["linkcolor=blue"]
date: "COMPoissonReg v0.8.0"
bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{A Vignette for the COMPoissonReg Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  prompt = TRUE,
  comment = ""
)
```

```{r setup, include = FALSE}
library(COMPoissonReg)
set.seed(1235)
```

\newpage

# Introduction
\label{sec:intro}

TBD:

- See the manual for a full specification of the public interface.
- The package uses `R` [@Rcore] (of course), `Rcpp` [@Eddelbuettel2013], and `numDeriv` [@numDeriv].
- We use `ggplot2` [@Wickham2016] here in the vignette, but not within the package.
- We could cite other CMP-related packages, like @combayes, @multicmp, @CompGLM, @compoisson, @cmpreg, @FungEtAl2020, and @SASProcCountreg2018.

# Conway Maxwell Poisson Distribution
\label{sec:cmp}

Let $Y \sim \text{CMP}(\lambda, \nu)$ be a Conway Maxwell Poisson (CMP) random variable with density
<!-- -->
\begin{align*}
f(y \mid \lambda, \nu) = \frac{\lambda^y}{(y!)^\nu Z(\lambda, \nu)}, \quad y \in \mathbb{N}, \quad
Z(\lambda, \nu) = \sum_{r=0}^\infty \frac{\lambda^r}{(r!)^\nu},
\end{align*}
<!-- -->
where $\lambda > 0$, $\nu > 0$, and $\mathbb{N}$ represents the nonnegative integers $\{ 0, 1, 2, \ldots\}$. Three notable special cases of $\text{CMP}(\lambda, \nu)$ help to demonstrate its flexibility in capturing dispersion.

a. The case $\nu = 1$ corresponds to $\text{Poisson}(\lambda)$. 

a. When $\lambda \in (0,1)$ and $\nu \rightarrow 0$, the $\text{CMP}(\lambda, \nu)$ distribution becomes $\text{Geometric}(\lambda)$ with density $f(y \mid \lambda) = \lambda^y (1 - \lambda)^{1-y}$ for $y \in \mathbb{N}$, which is overdispersed relative to Poisson. 

a. When $\nu \rightarrow \infty$, $\text{CMP}(\lambda, \nu)$ converges to a $\text{Bernoulli}(\lambda / (1 + \lambda))$ distribution which is underdispersed relative to Poisson.

@ShmueliEtAl2005 discusses some important properties of CMP as a count distribution, and @SellersShmueli2010 demonstrate the use of CMP in regression modeling.

## Normalizing Constant
\label{sec:cmp-normconst}
The normalizing constant $Z(\lambda, \nu)$ presents some challenges in the practical use of CMP models and has been a topic of interest in the CMP literature. In general, there is no simple closed form expression for the series $Z(\lambda, \nu)$. @ShmueliEtAl2005 give the approximation
<!-- -->
\begin{align}
Z(\lambda, \nu) &= \frac{ \exp(\nu \lambda^{1/\nu}) }{ \lambda^{(\nu-1)/2\nu} (2\pi)^{(\nu-1)/2} \nu^{1/2} }
\left\{ 1 + O(\lambda^{-1/\nu}) \right\}.
\label{eqn:approx}
\end{align}
<!-- -->
Approximations have been further studied and refined in subsequent literature; see for example @GillispieGreen2015, @DalyGaunt2016, and @GauntEtAl2019. The first term in \eqref{eqn:approx} emphasizes that the magnitude of $Z(\lambda, \nu)$ explodes as $\nu \rightarrow 0$ when $\lambda > 1$. For example, $Z(2, 0.075) \approx e^{780.515}$ is too large to store as a double-precision floating point number, and may evaluate to infinity if care is not taken. In contrast, $Z(\lambda, \nu) \rightarrow 1/(1 - \lambda)$ when $\lambda < 1$ and $\nu \rightarrow 0$.

In practice, the `COMPoissonReg` package does not place constraints on $\lambda$ and $\nu$, except to ensure that they are positive, so that their values are driven by the data or the user's selection. A hybrid strategy motivated by \eqref{eqn:approx} is taken by `COMPoissonReg`. Given a small tolerance $\delta > 0$, if $\lambda^{-1/\nu} < \delta$, the first term of \eqref{eqn:approx} dominates the second term, and we take
<!-- -->
\begin{align}
Z(\lambda, \nu) &\approx \frac{ \exp(\nu \lambda^{1/\nu}) }{ \lambda^{(\nu-1)/2\nu} (2\pi)^{(\nu-1)/2} \nu^{1/2} } \nonumber \\
&=\exp\left\{
\nu \lambda^{1/\nu} - \frac{\nu-1}{2\nu} \log \lambda - \frac{\nu-1}{2} \log(2\pi) - \frac{1}{2} \log \nu
\right\}.
\label{eqn:z-approx}
\end{align}
<!-- -->
as an approximation. Otherwise, the series is computed truncating the series to a finite number of terms, which is described next. In either case, computations are kept on the log-scale as much as possible to accommodate numbers with potentially very large magnitudes.

We approximate $Z(\lambda, \nu)$ by a finite summation $Z^{(M)}(\lambda, \nu) = \sum_{r=0}^M \lambda^r / (r!)^\nu$ if
<!-- --->
\begin{align}
\lambda^{-1/\nu} \geq \delta.
\label{eqn:can_trunc}
\end{align}
<!-- --->
The general approach for this is described in Appendix B of @ShmueliEtAl2005. Here we consider Stirling's approximation in particular; for an adequately
large $M$, Stirling's approximation can be applied to obtain bounds for the remainder with a closed form. @Feller1968 [, Section 2.9] gives the bound
<!-- -->
\begin{align*}
\sqrt{2\pi} n^{n + 1/2} e^{-n} e^{1 / (12n + 1)} < n! <
\sqrt{2\pi} n^{n + 1/2} e^{-n} e^{1 / (12n)}.
\end{align*}
<!-- -->
(This form is stated at <https://en.wikipedia.org/wiki/Stirling\%27s_approximation>.) Now we can obtain bounds which will be convenient in the following calculations.
<!-- -->
\begin{align*}
\sqrt{2\pi} n^{n + 1/2} e^{-n} \leq n! \leq e n^{n + 1/2} e^{-n}
\end{align*}
<!-- -->
by noting that $e^{1 / (12n + 1)} \geq 1$ for $n \geq 1$ and $\sqrt{2\pi} e^{1 / (12n)} \leq e$ for $n \geq 2$. We may then bound the truncation error for $Z^{(M)}(\lambda, \nu)$ using
<!-- -->
\begin{align}
\lvert Z(\lambda, \nu) - Z^{(M)}(\lambda, \nu) \rvert &= Z(\lambda, \nu) - Z^{(M)}(\lambda, \nu) \nonumber \\
&= \sum_{r=M+1}^\infty \frac{\lambda^r}{(r!)^\nu} \nonumber \\
&\leq \sum_{r=M+1}^\infty \frac{\lambda^r}{(2\pi)^{\nu/2} r^{\nu r + \nu/2} e^{-r \nu}} \nonumber \\
&\leq \sum_{r=M+1}^\infty \frac{\lambda^r}{(2\pi)^{\nu/2} (M+1)^{\nu r + \nu/2} e^{-r \nu}} \nonumber \\
&= (2\pi)^{-\nu/2} (M+1)^{-\nu/2} \sum_{r=M+1}^\infty \left( \frac{\lambda e^{\nu}}{(M+1)^{\nu}} \right)^r \label{eqn:geom} \\
&= (2\pi)^{-\nu/2} (M+1)^{-\nu/2} \sum_{r=0}^\infty \left( \frac{\lambda e^{\nu}}{(M+1)^{\nu}} \right)^{r+M+1} \nonumber \\
&= (2\pi)^{-\nu/2} (M+1)^{-\nu/2} \left( \frac{\lambda e^{\nu}}{(M+1)^{\nu}} \right)^{M+1} \frac{1}{1 - \frac{\lambda e^{\nu}}{(M+1)^{\nu}}} \nonumber \\
&=: \Delta_M, \nonumber
\end{align}
<!-- -->
assuming that $|\lambda e^\nu / (M+1)^\nu| < 1$ for convergence of the
geometric series in \eqref{eqn:geom}. To ensure this we choose $M$ at least
large enough so that
<!-- -->
\begin{align*}
\lambda e^\nu / (M+1)^\nu < 1 \iff
M > \lambda^{1/\nu} e - 1.
\end{align*}
<!-- -->
Consider bounding the relative error by a small given number $\epsilon > 0$:
<!-- -->
\begin{align*}
&\frac{\lvert Z(\lambda, \nu) - Z^{(M)}(\lambda, \nu) \rvert}{Z^{(M)}(\lambda, \nu)}
\leq \frac{\Delta_M}{Z^{(M)}(\lambda, \nu)}
< \epsilon.
\end{align*}
<!-- -->
This condition can be expressed on the log-scale using
<!-- -->
\begin{align}
\log \Delta_M - \log Z^{(M)}(\lambda, \nu) < \log \epsilon,
\label{eqn:adaptive-log-scale}
\end{align}
<!-- -->
where
<!-- -->
\begin{align*}
\log \Delta_M
= -\frac{\nu}{2} \log(2\pi)
- \nu \left( M+\frac{3}{2} \right) \log (M+1)
+ (M+1) (\nu + \log \lambda)
- \log\left(1 - \frac{\lambda e^\nu}{(M+1)^\nu} \right).
\end{align*}
<!-- -->
Therefore, we compute $Z^{(M)}(\lambda, \nu)$ until at least
$M > \lambda^{1/\nu} e - 1$, increasing $M$ and updating
$Z^{(M)}(\lambda, \nu)$ until \eqref{eqn:adaptive-log-scale} is satisfied.

The individual terms $\lambda^r / (r!)^\nu$ in the summation may be too large to store at their original scale. Therefore, summation is carried out at the log-scale, wherever possible, using the identity
<!-- -->
\begin{align}
\log(x + y) = \log x + \log(1 + \exp\{ \log y - \log x \});
\label{eqn:logadd}
\end{align}
<!-- -->
this is especially helpful when $0 < y << x$, as $\log x$ may be kept on the log-scale by the first term of the right-hand side of \eqref{eqn:logadd}, and the standard library function `log1p` may be used with the second term to accurately compute $\log(1 + \epsilon)$ for very small $\epsilon > 0$.


The tolerances $\delta$ and $\epsilon$ are specified in `COMPoissonReg` using global options `COMPoissonReg.hybrid.tol` and `COMPoissonReg.truncate.tol` respectively. Furthermore, the maximum value of $M$ is specified by the option `COMPoissonReg.ymax`. This is a safety measure that prevents very large computations unless the user opts to allow them. All options can be accessed using R's `getOption` function, with default values shown.

```{r}
getOption("COMPoissonReg.hybrid.tol")
getOption("COMPoissonReg.truncate.tol")
getOption("COMPoissonReg.ymax")
```

The `ncmp` function computes the $Z(\lambda, nu)$ and returns its value either on its original scale or the log-scale. The `method` argument specifies how the computation will be carried out. The argument `method = "approx"` corresponds to \ref{eqn:z-approx}, while `method = "trunc"` uses the truncated sum $Z^{(M)}(\lambda, \nu)$, and the default `method = "hybrid"` uses the hybrid strategy.

```{r}
ncmp(lambda = 1.5, nu = 1.2)
ncmp(lambda = 1.5, nu = 1.2, log = TRUE)
ncmp(lambda = 1.5, nu = 1.2, log = TRUE, method = "approx")
ncmp(lambda = 1.5, nu = 1.2, log = TRUE, method = "trunc")
```

The function `tcmp` returns the truncation value $M$ obtained from `ncmp` with `method = "trunc"`.

```{r}
print_warning = function(x) { print(strwrap(x), quote = FALSE) }
nu_seq = c(1, 0.5, 0.2, 0.1, 0.05, 0.03)
tryCatch({ tcmp(lambda = 1.5, nu = nu_seq) }, warning = print_warning)
```

Note that `tcmp` returns `1e6` for the last `nu` value of ``0.03`` because we have reached the maximum value specified by `COMPoissonReg.ymax = 1e6` before capturing a sufficient proportion of the normalizing constant using \eqref{eqn:adaptive-log-scale}. Here, support values with with non-negligible mass may be left out. Therefore, let us increase `COMPoissonReg.ymax` for the current session using R's `options` function and try again.

```{r}
options(COMPoissonReg.ymax = 3e6)
tcmp(lambda = 1.5, nu = c(1, 0.5, 0.2, 0.1, 0.05, 0.03))
```

Now that we have a somewhat robust computation for the normalizing constant, let us plot this interesting behavior when $\lambda > 1$ and $\nu$ decreases.

```{r, fig.width = 5, fig.height = 3, fig.align = "center", prompt = FALSE}
library(ggplot2)

nu_seq = seq(0.03, 1.5, length.out = 20)
norm1 = ncmp(lambda = 0.5, nu = nu_seq, log = TRUE)
norm2 = ncmp(lambda = 1.05, nu = nu_seq, log = TRUE)
norm3 = ncmp(lambda = 1.20, nu = nu_seq, log = TRUE)

ggplot() +
	geom_point(data = data.frame(x = nu_seq, y = norm1), aes(x = x, y = y), pch = 1) +
	geom_point(data = data.frame(x = nu_seq, y = norm2), aes(x = x, y = y), pch = 2) +
	geom_point(data = data.frame(x = nu_seq, y = norm3), aes(x = x, y = y), pch = 3) +
	xlab("nu") +
	ylab("log of normalizing constant") +
	theme_bw()
```
We see that `lambda = 1.2` (represented by $+$ symbols) reaches a value of $Z(\lambda, \nu) \approx e^{18.67}$ when `nu = 0.03`.

## Density, Generation, CDF, and Quantile Functions
\label{sec:cmp-dist}

The respective functions for CMP density, variate generation, CDF, and quantile functions are `dcmp`, `rcmp`, `pcmp` , and `qcmp`. Their usage is similar to distribution functions provided by the R `stats` package.

```{r}
dcmp(0, lambda = 10, nu = 0.9)
dcmp(0:17, lambda = 10, nu = 0.9, log = TRUE)
dcmp(c(0, 1, 2), lambda = c(10, 11, 12), nu = c(0.9, 1.0, 1.1), log = TRUE)
```

```{r}
rcmp(50, lambda = 10, nu = 0.9)
```

```{r}
pcmp(0:17, lambda = 10, nu = 0.9)
```

```{r}
qq = seq(0, 1, length.out = 10)
qcmp(qq, lambda = 10, nu = 0.9)
```

At present, all four functions compute the normalizing constant via Section \ref{sec:cmp-normconst}. The density `dcmp` uses the hybrid method, while `rcmp`, `pcmp`, and `qcmp` use the truncation method regardless of condition \eqref{eqn:can_trunc}. Variate generation, CDF, and quantiles are then computed on CMP as if it were a discrete distribution on the sample space $\{ 0, \ldots, M \}$. A warning is emitted in cases where they may not produce reliable results.

```{r}
print_warning = function(x) { print(strwrap(x), quote = FALSE) }
```

```{r}
tryCatch({ rcmp(1, lambda = 2, nu = 0.01) }, warning = print_warning)
```

```{r}
tryCatch({ pcmp(10, lambda = 2, nu = 0.01) }, warning = print_warning)
```

```{r}
tryCatch({ qcmp(0.5, lambda = 2, nu = 0.01) }, warning = print_warning)
```

As a quick test, let us ensure that the empirical density values, cumulative probabilities, and quantiles line up with the ones computed via `dcmp`, `pcmp`, and `qcmp`.

```{r, fig.width = 3, fig.height = 3, fig.align = "center", prompt = FALSE, fig.show = "hold"}
library(ggplot2)

n = 100000
lambda = 0.5
nu = 0.1
x = rcmp(n, lambda, nu)

xx = seq(-1, max(x))  ## Include -1 to make sure it gets probability zero
qq = seq(0, 1, length.out = 100)

fx = dcmp(xx, lambda, nu)
px = pcmp(xx, lambda, nu)
qx = qcmp(qq, lambda, nu)

qx_emp = quantile(x, probs = qq)

ggplot() +
	geom_bar(data = data.frame(x = x), aes(x = x, y = ..prop..), fill = "NA",
		col = "black") +
	geom_point(data = data.frame(x = xx, fx = fx), aes(x, fx)) +
	ylab("Proportion") +
	ggtitle("Empirical Density vs. dcmp") +
	theme_bw()

ggplot() +
	stat_ecdf(data = data.frame(x = x), aes(x), geom = "step") +
	geom_point(data = data.frame(x = xx, px = px), aes(x, px)) +
	ylab("Proportion") +
	ggtitle("Empirical CDF vs. pcmp") +
	theme_bw()

ggplot() +
	geom_point(data = data.frame(x = qq, qx_emp = qx_emp), aes(qq, qx_emp), pch = 1) +
	geom_point(data = data.frame(x = qq, qx = qx), aes(qq, qx), pch = 3) +
	xlab("Probability") +
	ylab("Quantile") +
	ggtitle("Empirical Quantiles vs. qcmp") +
	theme_bw()
```

Notice that the empirical and `qcmp` quantiles for probability $1.0$ do not match: this is because the truncation value $M$---which is returned by `qcmp`---is slightly larger than necessary, and none of the `n` draws have attained this value because the probability associated with it is extremely small. Because the support of CMP is $\mathbb{N}$, the actual quantile for probability $1.0$ is $\infty$ which can be expressed with no computation.

## Expected Value and Variance
\label{sec:cmp-ev}

For $Y \sim \text{CMP}(\lambda, \nu)$, we can consider computing the expectation and variance of $Y$ in two ways. First, if there is a moderately-sized $M$ where $\{ 0, \ldots, M \}$ contains all but a negligible amount of the mass of $Y$, we can compute the moments using
<!-- -->
\begin{align*}
\E(Y) = \sum_{y=0}^M y \frac{\lambda^y}{(y!)^\nu Z(\lambda, \nu)}, \quad
\E(Y^2) = \sum_{y=0}^M y^2 \frac{\lambda^y}{(y!)^\nu Z(\lambda, \nu)}, \quad
\Var(Y) = \E(Y^2) - [\E(Y)]^2.
\end{align*}
<!-- -->
Otherwise, a different approach is taken. Notice that the expected value is related to the first derivative of the log-normalizing constant via
<!-- -->
\begin{align*}
&\frac{\partial}{\partial \lambda} \log Z(\lambda, \nu)
= \frac{ \frac{\partial}{\partial \lambda} Z(\lambda, \nu) }{ Z(\lambda, \nu) }
= \frac{1}{Z(\lambda, \nu)} \sum_{y=0}^\infty y \frac{\lambda^{y-1}}{(y!)^\nu} \\
&\iff \E(Y) = \lambda \frac{\partial}{\partial \lambda} \log Z(\lambda, \nu).
\end{align*}
<!-- -->
Also
<!-- -->
\begin{align*}
&\frac{\partial^2}{\partial \lambda^2} \log Z(\lambda, \nu)
= \frac{ Z(\lambda, \nu) \frac{\partial^2}{\partial \lambda^2} Z(\lambda, \nu) - [\frac{\partial}{\partial \lambda} Z(\lambda, \nu)]^2}{ Z(\lambda, \nu)^2 }
= \frac{1}{Z(\lambda, \nu)} \sum_{y=0}^\infty y(y-1) \frac{\lambda^{y-2}}{(y!)^\nu} - \left[ \frac{\E(Y)}{\lambda} \right]^2 \\
&\iff \lambda^2 \frac{\partial^2}{\partial \lambda^2} \log Z(\lambda, \nu) = \E[Y(Y-1)] - [\E(Y)]^2 = \Var(Y) - \E(Y) \\
&\iff \Var(Y) = \lambda^2 \frac{\partial^2}{\partial \lambda^2} \log Z(\lambda, \nu) + \E(Y).
\end{align*}
<!-- -->
Therefore, we may use first and second derivatives of $\log Z(\lambda, \nu)$, with respect to $\lambda$, to compute $\E(Y)$ and $\Var(Y)$. To decide whether to use the truncation or the differentiation approaches, we check condition \eqref{eqn:can_trunc}: if it holds, there is a sufficiently small $M$ to use the truncation method.

The `ecmp` and `vcmp` functions implement this computation of the expectation and variance, respectively.

```{r}
ecmp(lambda = 10, nu = 1.2)
ecmp(lambda = 1.5, nu = 0.5)
ecmp(lambda = 1.5, nu = 0.05)
ecmp(lambda = 1.5, nu = 0.05, method = "trunc")
ecmp(lambda = 1.5, nu = 0.05, method = "approx")
```

```{r}
vcmp(lambda = 10, nu = 1.2)
vcmp(lambda = 1.5, nu = 0.5)
vcmp(lambda = 1.5, nu = 0.05)
vcmp(lambda = 1.5, nu = 0.05, method = "trunc")
vcmp(lambda = 1.5, nu = 0.05, method = "approx")
```

As with the `ncmp` function, `method = "approx"` explicitly requests the differentiation approach, `method = "trunc"` requests truncation, and the default `method = "hybrid"` uses condition \eqref{eqn:can_trunc} to decide.

Provided that an enormously large truncation value $M$ is not required, other moments can be computed using `tcmp` by truncated sums.

```{r}
M = tcmp(lambda = 1.5, nu = 0.05)
xx = seq(0, M)

print(M)
sum(xx^3 * dcmp(xx, lambda, nu))    # E(X^3)
sum(xx^4 * dcmp(xx, lambda, nu))    # E(X^4)
```

# Zero-Inflated Conway Maxwell Poisson Distribution

Let $S \sim \text{Bernoulli}(p)$ and $T \sim \text{CMP}(\lambda, \nu)$ be independent random variables. Then $Y = (1-S) T$ has Zero-Inflated Conway Maxwell Poisson distribution $\text{ZICMP}(\lambda, \nu, p)$ with density
<!-- -->
\begin{align*}
f(y \mid \lambda, \nu, p) =  (1 - p) \frac{\lambda^{y}}{(y!)^{\nu} Z(\lambda, \nu)}
+ p \cdot \ind(y = 0), \quad y \in \mathbb{N}.
\end{align*}
<!-- -->
Like CMP, several interesting special cases are obtained by varying $\nu$. 

a. Taking $\nu = 1$ corresponds to Zero-Inflated Poisson $\text{ZIP}(\lambda, p)$ with density $f(y \mid \lambda, p) = (1 - p) e^{-\lambda} \lambda^{y} / y! + p \cdot \ind(y = 0)$.

a. When $\lambda \in (0,1)$ and $\nu \rightarrow 0$, $\text{ZICMP}(\lambda, \nu)$ converges to a Zero-Inflated Geometric distribution with density $f(y \mid \lambda, p) = (1 - p) \lambda^y (1 - \lambda)^{1-y} + p \cdot \ind(y = 0)$ for $y \in \mathbb{N}$.

a. Finally, when $\nu \rightarrow \infty$, $\text{ZICMP}(\lambda, \nu, p)$ converges to a ``Zero-Inflated Bernoulli'' distribution with density 
<!-- -->
\begin{math}
f(y \mid \lambda, p) = (1 - p) \left[ \lambda/(1+\lambda) \right]^y \left[ 1/(1+\lambda) \right]^{1-y} + p \cdot \ind(y = 0).
\end{math}

In case (c) the $\lambda$ and $p$ parameters are not identifiable, and users may want to avoid ZICMP in data analyses with extreme underdispersion. @SellersRaim2016 provide more discussion on ZICMP and its use in regression modeling.

## Density, Generation, CDF, and Quantile Functions

There is a close relationship between the CMP and ZICMP distributions, as we have seen from construction of ZICMP. The relationship between the densities and variate generation mechanisms was given earlier in this section. Furthermore, let $F(x \mid \lambda, \nu)$ and $F^{-}(\phi \mid \lambda, \nu)$ denote the CDF and quantile function of $\text{CMP}(\lambda, \nu)$ and $F(x \mid \lambda, \nu, p)$ and $F^{-}(\phi \mid \lambda, \nu, p)$ denote the CDF and quantile function of $\text{ZICMP}(\lambda, \nu, p)$. We have
<!-- -->
\begin{align*}
F(x \mid \lambda, \nu, p) = (1-p) F(x \mid \lambda, \nu) + p \cdot \ind(x \geq 0)
\end{align*}
<!-- -->
and, for a given $\phi \in [0,1]$,
<!-- -->
\begin{align*}
F^{-1}(\phi \mid \lambda, \nu, p)
&= \inf\{ x \in \mathbb{N} : F(x \mid \lambda, \nu, p) \geq \phi \} \\
&= \inf\{ x \in \mathbb{N} : (1-p) F(x \mid \lambda, \nu) + p \cdot \ind(x \geq 0) \geq \phi \} \\ 
&= \inf\{ x \in \mathbb{N} : F(x \mid \lambda, \nu) \geq (\phi - p) / (1 - p) \} \\
&= F^{-1}\left( \frac{\phi - p}{1 - p} \mid \lambda, \nu \right)
\end{align*}

Therefore, the 

The respective functions for ZICMP density, variate generation, CDF, and quantile functions are `dzicmp`, `rzicmp`, `pzicmp` , and `qzicmp`. 



They make use of the CMP implementation described in Section \ref{sec:cmp} such as the criteria to either truncate or approximation the normalizing constant.

Here are some examples showing the `dzicmp`, `rzicmp`, `pzicmp`, and `qzicmp` functions.

```{r}
qq = seq(0, 1, length.out = 20)

rzicmp(20, lambda = 1.5, nu = 0.2, p = 0.25)
dzicmp(c(0, 1, 2), lambda = 1.5, nu = 0.2, p = 0.25)
pzicmp(c(0, 1, 2), lambda = 1.5, nu = 0.2, p = 0.25)
qzicmp(qq, lambda = 1.5, nu = 0.2, p = 0.25)
```

Let us repeat the test from Section \ref{sec:cmp-dist} to ensure that the empirical density values, cumulative probabilities, and quantiles line up with the ones computed via `dzicmp`, `pzicmp`, and `qzicmp`.

```{r, fig.width = 3, fig.height = 3, fig.align = "center", prompt = FALSE, fig.show = "hold"}
library(ggplot2)

n = 100000
lambda = 0.5
nu = 0.1
p = 0.5
x = rzicmp(n, lambda, nu, p)

xx = seq(-1, max(x))  ## Include -1 to make sure it gets probability zero
qq = seq(0, 1, length.out = 100)

fx = dzicmp(xx, lambda, nu, p)
px = pzicmp(xx, lambda, nu, p)
qx = qzicmp(qq, lambda, nu, p)

qx_emp = quantile(x, probs = qq)

ggplot() +
	geom_bar(data = data.frame(x = x), aes(x = x, y = ..prop..), fill = "NA",
		col = "black") +
	geom_point(data = data.frame(x = xx, fx = fx), aes(x, fx)) +
	ylab("Proportion") +
	ggtitle("Empirical Density vs. dzicmp") +
	theme_bw()

ggplot() +
	stat_ecdf(data = data.frame(x = x), aes(x), geom = "step") +
	geom_point(data = data.frame(x = xx, px = px), aes(x, px)) +
	ylab("Proportion") +
	ggtitle("Empirical CDF vs. pzicmp") +
	theme_bw()

ggplot() +
	geom_point(data = data.frame(x = qq, qx_emp = qx_emp), aes(qq, qx_emp), pch = 1) +
	geom_point(data = data.frame(x = qq, qx = qx), aes(qq, qx), pch = 3) +
	xlab("Probability") +
	ylab("Quantile") +
	ggtitle("Empirical Quantiles vs. qzicmp") +
	theme_bw()
```

The empirical quantiles match the results of `qzicmp`, except for probability $1.0$ which is quite different. This is a consequence of the truncation method which has been used; see the note in Section \ref{sec:cmp-dist} about `qcmp`, where this also occurs but to a lesser degree.

## Expectation and Variance
The expected value and variance of $Y \sim \text{ZICMP}(\lambda, \nu, p)$ are apparent from the construction $Y = (1-S) T$ given earlier in this section. Namely,
<!-- -->
\begin{align*}
\E(Y) = (1-p) \E(T)
\quad \text{and} \quad
\Var(Y) = (1-p) \left\{ \Var(T) + p[\E(T)]^2 \right\}
\end{align*}
<!-- -->
may be obtained using formulas for iterated conditional expectations and variances. They may be invoked in `COMPoissonReg` using the `ezicmp` and `vzicmp` functions respectively. These make use of the logic described in Section \ref{sec:cmp-ev} to compute $\E(T)$ and $\Var(T)$.

```{r}
ezicmp(lambda = 1.5, nu = 0.5, p = 0.1)
ezicmp(lambda = 1.5, nu = 0.5, p = c(0.1, 0.2, 0.5))
```

```{r}
vzicmp(lambda = 1.5, nu = 0.5, p = 0.1)
vzicmp(lambda = 1.5, nu = 0.5, p = c(0.1, 0.2, 0.5))
```

# Regression Modeling with CMP and ZICMP

Suppose there are $n$ subjects with outcomes $y_1, \ldots, y_n \in \mathbb{N}$ and covariates $\vec{x}_i \in \mathbb{R}^{d_1}$, $\vec{s}_i \in
\mathbb{R}^{d_2}$, and $\vec{w}_i \in \mathbb{R}^{d_3}$ for $i = 1, \ldots, n$. The `COMPoissonReg` package fits both CMP and ZICMP regression models.

The CMP regression model assumes that
<!-- -->
\begin{align*}
Y_i \indep \text{ZICMP}(\lambda_i, \nu_i, p_i), \quad i = 1, \ldots, n,
\end{align*}
<!-- -->
where $\log \lambda_i = \vec{x}_i^\top \vec{\beta}$ and $\log \nu_i = \vec{s}_i^\top \vec{\gamma}$. Writing $\btheta = (\vec{\beta}, \vec{\gamma})$, the likelihood is
<!-- -->
\begin{align}
L(\btheta) =
\prod_{i=1}^n \left[
\frac{\lambda_i^{y_i}}{(y_i!)^{\nu_i} Z(\lambda_i, \nu_i)}
\right].
\label{eqn:likelihood-cmp}
\end{align}
<!-- -->
The ZICMP regression model assumes that
<!-- -->
\begin{align*}
Y_i \indep \text{ZICMP}(\lambda_i, \nu_i, p_i), \quad i = 1, \ldots, n,
\end{align*}
<!-- -->
where $\log \lambda_i = \vec{x}_i^\top \vec{\beta}$, $\log \nu_i = \vec{s}_i^\top \vec{\gamma}$, and $\logit p_i = \vec{w}_i^\top \vec{\zeta}$. Writing $\btheta = (\vec{\beta}, \vec{\gamma}, \vec{\zeta})$, the likelihood is
<!-- -->
\begin{align}
L(\btheta) =
\prod_{i=1}^n \left[(1 - p_i)
\frac{\lambda_i^{y_i}}{(y_i!)^{\nu_i} Z(\lambda_i, \nu_i)}
+ p_i \ind(y_i = 0)
\right].
\label{eqn:likelihood-zicmp}
\end{align}
<!-- -->
We will write $d = d_1 + d_2 + d_3$ for the total dimension of $\btheta$. The `glm.cmp` function is provided to handle both cases.

```{r, eval = FALSE, prompt = FALSE}
out = glm.cmp(formula.lambda, formula.nu = ~ 1, formula.p = NULL,
	data = NULL, beta.init = NULL, gamma.init = NULL, zeta.init = NULL, ...)
```

The interface contains three formulas: `formula.lambda` specifies the regression $\vec{x}_i^\top \vec{\beta}$ used for $\lambda_i$, while `formula.nu` and `formula.p` correspond to $\vec{s}_i^\top \vec{\gamma}$ for $\nu_i$ and $\vec{w}_i^\top \vec{\zeta}$ for $p_i$, respectively. ZICMP regression is fit when `formula.p` is set to something other than its default `NULL` value; otherwise, CMP regression is assumed. The `data` argument accepts the data as an explicit argument. The `beta.init`, `gamma.init`, and `zeta.init` allow initial values to be pass to the optimizer for fitting. See the package manual for details about the `glm.cmp` interface.

`COMPoissonReg` uses `optim` to compute the maximum likelihood estimate (MLE) $\hat{\btheta}$ for $\btheta$ under the specified model. Two global options are provided to influence how `COMPoissonReg` invokes `optim`; here are their default values.

```{r}
getOption("COMPoissonReg.optim.method")
getOption("COMPoissonReg.optim.control")
```

The string `COMPoissonReg.optim.method` is used as the `method` argument of `optim`, while the list `COMPoissonReg.optim.control` is used as the `control` argument. For the latter, note that any entry for `fnscale` is ignored by `COMPoissonReg`.

The covariance of $\hat{\btheta}$ is estimated by $\hat{\vec{V}}(\btheta) = -[\vec{H}(\hat{\btheta})]^{-1}$, where $\vec{H}(\btheta) = \frac{\partial^2}{\partial \btheta \partial \btheta^\top} \log L(\btheta)$ is the Hessian of the log-likelihood computed by `optim`. The standard error for coefficient $\theta_j$ in $\btheta$ is then obtained as the square root of the $j$th diagonal of $\hat{\vec{V}}(\btheta)$.

We will now illustrate use of the regression tools using two examples whose data are included in the package. Note that these demonstrations are not intended to be complete regression analyses, and results may be slightly different than previously published analyses due to differences in the computations.

## CMP Regression
\label{sec:cmp-reg}

The `freight` dataset [@KutnerEtAl2003] was analyzed using CMP regression and found to exhibit underdispersion by @SellersShmueli2010. The data describe $n = 10$ instances where 1,000 ampules were transported via air shipment. The variable `broken` is the outcome of interest, and describes the number of broken ampules in each shipment. The covariate `transfers` describes the number of times the carton was transferred from one aircraft to another during the shipment.

Let us load and view the dataset.

```{r}
data(freight)
print(freight)
```

Before fitting a CMP regression, let us fit a standard Poisson model $Y_i \indep \text{Poisson}(\lambda_i)$ with 
<!-- -->
\begin{align*}
\log \lambda_i = \beta_0 + \beta_1 \cdot \text{transfers}_i.
\end{align*}
<!-- -->
This can be carried out with the standard `glm` function.

```{r}
glm.out = glm(broken ~ transfers, data = freight, family = poisson)
summary(glm.out)
```

Next, let us fit a similar CMP regression model with
<!-- -->
\begin{align*}
&\log \lambda_i = \beta_0 + \beta_1 \cdot \text{transfers}_i, \\
&\log \nu_i = \gamma_0,
\end{align*}
<!-- -->
using only an intercept for $\nu_i$.

```{r}
cmp.out = glm.cmp(broken ~ transfers, data = freight)
print(cmp.out)
```

Notice that the coefficients used in the $\lambda_i$ formula are prefixed with an `X:` label, while an `S:` label is used for coefficients of the $\nu_i$ formula. Similarly to the `glm` output, the output of `glm.cmp` displays several quantities for each coefficient $\theta_j$ for $j = 1, \ldots, d$: a point estimate $\hat{\theta}_j$, an associated standard error $\widehat{\text{SE}}(\hat{\theta}_j)$, a z-value $z_j = \hat{\theta}_j / \widehat{\text{SE}}(\hat{\theta}_j)$, and a p-value $2\Phi(-|z_j|)$ for the test $H_0: \theta_j = 0$ versus $H_0: \theta_j \neq 0$. Here, $\Phi$ is the CDF of the standard normal distribution. Because an intercept-only formula was specified for $\nu_i$, an estimate and associated standard error are displayed for $\hat{\nu} = \exp(\hat{\gamma})$ which does not vary with $i$. Here we see evidence of underdispersion with $\hat{\nu} > 1$. A test for equidispersion is displayed to determine whether there is a significant amount of over or underdispersion in the data. In particular, a likelihood ratio test is used to decide whether $H_0: \vec{\gamma} = \vec{0}$ versus $H_0: \vec{\gamma} \neq \vec{0}$. The test statistic is displayed along with the degrees of freedom and associated p-value. Here we have fairly strong evidence to reject the null hypthesis of equidispersion.

The CMP model shows a good improvement in AIC over the Poisson model. We may alternatively fit
<!-- -->
\begin{align*}
\log \nu_i = \gamma_0 + \gamma_1 \cdot \text{transfers}_i.
\end{align*}
<!-- -->
using the following call to `glm.cmp`.

```{r}
cmp2.out = glm.cmp(broken ~ transfers, formula.nu = ~ transfers, data = freight)
print(cmp2.out)
```

Model `cmp2.out` provides a slight improvement over `cmp.out` in AIC and BIC, but we will proceed with `cmp.out` in the interest of simplicity.

Let us increase the trace level to watch the optimization.

```{r}
options(COMPoissonReg.optim.control = list(maxit = 5, trace = 3, REPORT = 1))
cmp3.out = glm.cmp(broken ~ transfers, data = freight)
```

We will return to the default options before we proceed.

```{r}
options(COMPoissonReg.optim.control = list(maxit = 150))
```

Data from the local environment may be passed to the `glm.cmp` function without explicitly using the `data` argument.

```{r}
y = freight$broken
x = freight$transfers
glm.cmp(y ~ x)
```

In a count regression model, it may be desirable to include offset terms such as
<!-- -->
\begin{align*}
&\log \lambda_i = \vec{x}_i^\top \vec{\beta} + \text{offx}_i,
&\log \nu_i = \vec{s}_i^\top \vec{\beta} + \text{offs}_i.
\end{align*}
<!-- -->
An `offset` term may be used in regression formulas to accomplish this.

```{r}
freight$offx = 13
freight$offs = 1
glm.cmp(broken ~ transfers + offset(offx), data = freight)
glm.cmp(broken ~ transfers + offset(offx), formula.nu = ~1 + offset(offs), data = freight)
```

For users who wish to bypass the formula interface and prepare the design matrices manually, a "raw" interface is also provided to the regression functionality.

```{r}
y = freight$broken
X = model.matrix(~ transfers, data = freight)
S = model.matrix(~ 1, data = freight)
off.x = rep(13, nrow(freight))
off.s = rep(1, nrow(freight))
glm.cmp.raw(y, X, S, off.x = off.x, off.s = off.s)
```

Several accessors are provided to extract results from the output object.

```{r}
coef(cmp.out)  ## Estimates of theta
nu(cmp.out)    ## Estimates of nu = (nu_1, ..., nu_n)
vcov(cmp.out)  ## Estimated covariance matrix of theta hat
sdev(cmp.out)  ## Standard deviations from vcov(...) diagonals
```

The `predict` function computes $\hat{y}_i = \E(Y_i)$ evaluated at the estimate $\hat{\btheta}$ using the method described in Section \ref{sec:cmp-ev}. Here we request fitted values for the sample used in the fit.

```{r}
predict(cmp.out)
```

We can also request fitted values for new covariate values.

```{r, fig.width = 3, fig.height = 3, fig.align = "center"}
new.data = data.frame(transfers = 0:10)
y.hat.new = predict(cmp.out, newdata = new.data)
ggplot() +
	geom_point(data = new.data, aes(transfers, y.hat.new)) +
	xlab("Number of transfers") +
	ylab("Predicted number broken") +
	theme_bw()
```

The `leverage` function computes the diagonal entries of a "hat" matrix which can be formulated in CMP regression. These can be used to diagnose influential observations. For details, see Section 3.6 of @SellersShmueli2010.

```{r}
leverage(cmp.out)
```

The `residuals` function provides either raw or quantile-based residuals [@DunnSmyth1996]. Raw residuals $y_i - \hat{y}_i$  generally do not work well with traditional regression diagnostics, such as Q-Q plots, in a CMP regression setting. Quantile-based residuals often produce interpretable diagnostics; however, a random element is used in the computation of quantile residuals for discrete distributions. This aids interpretability but gives slightly different residual values each time they are computed. See @DunnSmyth1996 for details.

```{r}
res.raw = residuals(cmp.out)
res.qtl = residuals(cmp.out, type = "quantile")
```

Pearson residuals may be preferred over raw residuals for diagnostics; these can be obtained by standardizing raw residuals using leverage values and variance estimates. (TBD: Kim may want to check this).

```{r}
# TBD: Need a nice way to retrieve lambda, nu, etc. How about a fitted function? It could take a "what" argument and return ecmp, lambda, nu, or p
# TBD: Maybe get rid of nu? Or keep it for legacy?
lambda.hat = exp(cmp.out$X %*% cmp.out$beta)
vv = vcmp(lambda.hat, nu(cmp.out))
hh = leverage(cmp.out)
res.pearson = res.raw / sqrt(vv*(1-hh))
```

Here we plot fitted values versus both sets of residuals and Q-Q plots of both residuals.

```{r, fig.width = 3, fig.height = 3, fig.align = "center", prompt = FALSE, fig.show = "hold"}
plot.fit.res = function(y.hat, res) {
	ggplot(data.frame(y = y.hat, res = res)) +
		geom_point(aes(y, res)) +
		xlab("Fitted") +
		ylab("Residual") +
		theme_bw()
}

plot.qq.res = function(res) {
	ggplot(data.frame(res = res), aes(sample = res)) +
		stat_qq() +
		stat_qq_line() +
		theme_bw()
}

y.hat = predict(cmp.out)

plot.fit.res(y.hat, res.raw) + ggtitle("Fitted vs Raw Residuals")
plot.qq.res(res.raw) + ggtitle("Q-Q Plot of Raw Residuals")

plot.fit.res(y.hat, res.pearson) + ggtitle("Fitted vs Pearson Residuals")
plot.qq.res(res.pearson) + ggtitle("Q-Q Plot of Pearson Residuals")

plot.fit.res(y.hat, res.qtl) + ggtitle("Fitted vs Quantile Residuals")
plot.qq.res(res.qtl) + ggtitle("Q-Q Plot of Quantile Residuals")
```

In this example, with only `r nrow(freight)` observations, it is difficult to see an advantage of using quantile residuals; the benefit will be more apparent in Section \ref{sec:zicmp-reg}. One benefit of raw residuals is that they may be used to compute a mean-squared error.

```{r}
mean(res.raw^2)
```

To access the results of the equidispersion test shown in the output of `cmp.out`, we may use the `equitest` accessor function.

```{r}
equitest(cmp.out)
```

The `deviance` function computes the deviance quantities $D_i = -2 [\log L_i(\hat{\btheta}) - \log L_i(\tilde{\btheta}_i)]$ for $i = 1, \ldots, n$, where $L_i(\btheta)$ is the term of the likelihood corresponding to the $i$th observation, $\hat{\btheta}$ is the MLE computed under the full likelihood $L(\btheta) = \prod_{i=1}^n L_i(\btheta)$, and $\tilde{\btheta}_i$ is the maximizer of $L_i(\btheta)$.

```{r}
deviance(cmp.out)
```

The `parametric.bootstrap` function carries out a parametric bootstrap with $R$ repetitions. Using the fitted MLE $\hat{\btheta}$, bootstrap samples $\vec{y}^{(r)} = (y_1^{(r)}, \ldots, y_n^{(r)})$ are drawn from likelihood $L(\hat{\btheta})$ for $r = 1, \ldots, R$. Estimate $\hat{\btheta}^{(r)}$ is fitted from bootstrap sample $\vec{y}^{(r)}$. An $R \times (d_1 + d_2 + d_3)$ matrix is returned whose $r$th row is $\hat{\btheta}^{(r)}$.

```{r}
cmp.boot = parametric.bootstrap(cmp.out, reps = 100)
head(cmp.boot)
```

We used $R = `r nrow(cmp.boot)`$ in the display above to keep vignette computations small, but a larger $R$ may be desired in practice. The bootstrap repetitions can be used, for example, to compute 95% confidence intervals for each of the coefficients.

```{r}
apply(cmp.boot, 2, quantile, c(0.025,0.975))
```

## ZICMP Regression
\label{sec:zicmp-reg}

The `couple` dataset [@LoeysEtAl2012] was analyzed with ZICMP regression in @SellersRaim2016 and found to exhibit overdispersion. The data concern the separation trajectories of $n = 387$ couples. The variable `UPB` records the number of unwanted pursuit behavior perpetrations and is considered the outcome of interest. Included covariates are the binary variable `EDUCATION`, which is 1 if at least a bachelor's degree was attained, and a continuous variable `ANXIETY` which measures anxious attachment. A zero-inflated count model is considered for this data because 246 of the 387 records have an outcome of `UPB = 0`.

First let us load and view the first few records in the dataset.

```{r}
data(couple)
head(couple)
```

As a preliminary model, let us first fit a standard Poisson model $Y_i \indep \text{Poisson}(\lambda_i)$ with
<!-- -->
\begin{align*}
\log \lambda_i = \beta_0 + \beta_1 \cdot \text{EDUCATION}_i + \beta_2 \cdot \text{ANXIETY}_i.
\end{align*}
<!-- -->
We may use the standard `glm` function.

```{r}
glm.out = glm(UPB ~ EDUCATION + ANXIETY, data = couple, family = poisson)
summary(glm.out)
```

Now consider a ZICMP regression with
<!-- -->
\begin{align*}
&\log \lambda_i = \beta_0 + \beta_1 \cdot \text{EDUCATION}_i + \beta_2 \cdot \text{ANXIETY}_i, \\
&\log \nu_i = \gamma_0, \\
&\logit p_i = \zeta_0 + \zeta_1 \cdot \text{EDUCATION}_i + \zeta_2 \cdot \text{ANXIETY}_i.
\end{align*}
<!-- -->
We use the `glm.cmp` function as follows.

```{r}
zicmp.out = glm.cmp(UPB ~ EDUCATION + ANXIETY,
	formula.nu = ~ 1,
	formula.p = ~ EDUCATION + ANXIETY,
 	data = couple)
print(zicmp.out)
```

The AIC of the ZICMP model is drastically smaller than the Poisson model, indicating a greatly improved fit. The estimate for $\gamma_0$ is a large negative number, which can sometimes lead to issues with infinite values when using the `L-BFGS-B` optimization method. Let us switch to `BFGS` for the remainder of this example.

```{r}
options(COMPoissonReg.optim.method = "BFGS")
```

Let us consider changing the regression on $\nu_i$ to
<!-- -->
\begin{align*}
\log \nu_i = \gamma_0 + \gamma_1 \cdot \text{EDUCATION}_i + \gamma_2 \cdot \text{ANXIETY}_i
\end{align*}
<!-- -->
as well.

```{r}
zicmp1.out = glm.cmp(UPB ~ EDUCATION + ANXIETY,
	formula.nu = ~ EDUCATION + ANXIETY,
	formula.p = ~ EDUCATION + ANXIETY,
 	data = couple)
print(zicmp1.out)
```

The AIC of model `zicmp1.out` is not much smaller than that of `zicmp.out`, so we will proceed with `zicmp.out`. From here, much of the interface is similar to CMP regression discussed in Section \ref{sec:cmp-reg}.

Here is an example using the raw interface.

```{r}
y = couple$UPB
X = model.matrix(~ EDUCATION + ANXIETY, data = couple)
S = model.matrix(~ 1, data = couple)
W = model.matrix(~ EDUCATION + ANXIETY, data = couple)
glm.zicmp.raw(y, X, S, W, beta.init = zicmp.out$beta,
	gamma.init = zicmp.out$gamma, zeta.init = zicmp.out$zeta)
```



```{r}
coef(zicmp.out)             ## Estimates of theta
head(nu(zicmp.out))         ## Estimates of nu = (nu_1, ..., nu_n)
vcov(zicmp.out)             ## Estimated covariance matrix of theta hat
sdev(zicmp.out)             ## Standard deviations from vcov(...) diagonals
y.hat = predict(zicmp.out)  ## Fitted values based on the expectation
equitest(zicmp.out)         ## Likelihood ratio test for H_0: gamma = 0
```

In this example, we can see the benefit of using quantile residuals for diagnostic plots.

```{r, fig.width = 3, fig.height = 3, fig.align = "center", prompt = FALSE, fig.show = "hold"}
plot.fit.res = function(y.hat, res) {
	ggplot(data.frame(y = y.hat, res = res)) +
		geom_point(aes(y, res)) +
		xlab("Fitted") +
		ylab("Residual") +
		theme_bw()
}

plot.qq.res = function(res) {
	ggplot(data.frame(res = res), aes(sample = res)) +
		stat_qq() +
		stat_qq_line() +
		theme_bw()
}

res.raw = residuals(zicmp.out, type = "raw")
res.qtl = residuals(zicmp.out, type = "quantile")

plot.fit.res(y.hat, res.raw) + ggtitle("Fitted vs Raw Residuals")
plot.qq.res(res.raw) + ggtitle("Q-Q Plot of Raw Residuals")

plot.fit.res(y.hat, res.qtl) + ggtitle("Fitted vs Quantile Residuals")
plot.qq.res(res.qtl) + ggtitle("Q-Q Plot of Quantile Residuals")
```

Here is an example of computing fitted values to new covariate data.

```{r}
new.data = data.frame(EDUCATION = round(1:20 / 20), ANXIETY = seq(-3,3, length.out = 20))
predict(zicmp.out, newdata = new.data)
head(y.hat.new)
```

As with CMP regression, a `parametric.bootstrap` function is provided for convenience to obtain a bootstrap sample $\hat{\btheta}^{(r)}$ of $\btheta$ based on the estimate $\hat{\btheta}$. Because it is too time consuming to run this example within the vignette, we show the code without output below. Again, we may use the bootstrap samples to construct a 95% confidence interval for each of the coefficients.

```{r, eval = FALSE}
zicmp.boot = parametric.bootstrap(zicmp.out, reps = 100)
head(zicmp.boot)
apply(zicmp.boot, 2, quantile, c(0.025,0.975))
```

# References

